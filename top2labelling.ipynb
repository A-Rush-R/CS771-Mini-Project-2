{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('dataset', 'part_one_dataset', 'train_data')\n",
    "eval_dir = os.path.join('dataset', 'part_one_dataset', 'eval_data')\n",
    "train_path = os.path.join(train_dir, '1_train_data.tar.pth')\n",
    "eval_path = os.path.join(eval_dir, '1_eval_data.tar.pth')\n",
    "\n",
    "t = torch.load(train_path, weights_only = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LWP Model using Distance Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances, manhattan_distances\n",
    "\n",
    "class LWP:\n",
    "    \"\"\"Learning Vector Prototypes with configurable distance function\"\"\"\n",
    "    \n",
    "    DISTANCE_FUNCTIONS = {\n",
    "        'euclidean': lambda x, y: np.linalg.norm(x - y),\n",
    "        'cosine': lambda x, y: cosine_distances(x.reshape(1, -1), y.reshape(1, -1))[0][0],\n",
    "        'manhattan': lambda x, y: manhattan_distances(x.reshape(1, -1), y.reshape(1, -1))[0][0],\n",
    "        'minkowski': lambda x, y, p=2: np.power(np.sum(np.power(np.abs(x - y), p)), 1/p)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, distance_metric='euclidean', **distance_params):\n",
    "        \"\"\"\n",
    "            distance_params (dict): Additional parameters for the distance function\n",
    "        \"\"\"\n",
    "        self.prototypes = {}\n",
    "        self.class_counts = {i: 0 for i in range(10)}\n",
    "        \n",
    "        if callable(distance_metric):\n",
    "            self.distance_fn = distance_metric\n",
    "        elif distance_metric in self.DISTANCE_FUNCTIONS:\n",
    "            if distance_metric == 'minkowski':\n",
    "                p = distance_params.get('p', 2)\n",
    "                self.distance_fn = lambda x, y: self.DISTANCE_FUNCTIONS[distance_metric](x, y, p)\n",
    "            else:\n",
    "                self.distance_fn = self.DISTANCE_FUNCTIONS[distance_metric]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {distance_metric}. \" \n",
    "                           f\"Available metrics: {list(self.DISTANCE_FUNCTIONS.keys())}\")\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            samples = features[labels == label]\n",
    "            num_samples = len(samples)\n",
    "            \n",
    "            if label not in self.prototypes:  # Original condition was: if label not in self.prototypes\n",
    "                self.prototypes[label] = samples.mean(axis=0)\n",
    "                self.class_counts[label] = len(samples)\n",
    "            else:\n",
    "                self.class_counts[label] += len(samples)\n",
    "                self.prototypes[label] = (\n",
    "                    (self.class_counts[label] - num_samples) / self.class_counts[label] * self.prototypes[label] +\n",
    "                    num_samples / self.class_counts[label] * samples.mean(axis=0)\n",
    "                )\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "    def predict(self, features):\n",
    "        preds = []\n",
    "        for feature in features:\n",
    "            distances = {\n",
    "                label: self.distance_fn(feature, proto)\n",
    "                for label, proto in self.prototypes.items()\n",
    "            }\n",
    "            preds.append(min(distances, key=distances.get))\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_proba(self, features):\n",
    "        \"\"\"\n",
    "        Predict probabilities (normalized distances to prototypes).\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the data points.\n",
    "        Returns:\n",
    "            np.array: Probabilities for each class.\n",
    "        \"\"\"\n",
    "        prob_list = []\n",
    "        for feature in features:\n",
    "            # Calculate distances to all prototypes\n",
    "            \n",
    "            # for label, proto in enumerate(self.prototypes.values()) :\n",
    "            #     print('shape of proto is' , proto.shape)\n",
    "            distances = {\n",
    "                label: self.distance_fn(feature, proto) for label, proto in enumerate(self.prototypes.values())\n",
    "            }\n",
    "            \n",
    "            # Convert distances to probabilities\n",
    "            exp_neg_distances = np.exp(-np.array(list(distances.values())))  # Exponential of negative distances\n",
    "            probabilities = exp_neg_distances / exp_neg_distances.sum()  # Normalize to sum to 1\n",
    "            \n",
    "            prob_list.append(probabilities)\n",
    "        \n",
    "        return np.vstack(prob_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity and select top samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_samples(embeddings, centroids, top_percentage=0.5):\n",
    "    \"\"\"\n",
    "    Select top-k% samples with highest cosine similarity to centroids\n",
    "    and include top-2 pseudo-labels.\n",
    "    Args:\n",
    "        embeddings (np.array): The data embeddings.\n",
    "        centroids (np.array): Prototypes or centroids for each class.\n",
    "        top_percentage (float): Percentage of top samples to select (0 < top_percentage <= 1).\n",
    "    Returns:\n",
    "        tuple: Top-k% embeddings, top-1 pseudo-labels, and top-2 pseudo-labels.\n",
    "    \"\"\"\n",
    "    if not (0 < top_percentage <= 1):\n",
    "        raise ValueError(\"top_percentage must be between 0 and 1.\")\n",
    "\n",
    "    # Compute similarity scores\n",
    "    similarities = cosine_similarity(embeddings, centroids)\n",
    "\n",
    "    # Compute the number of samples to select (50% of total embeddings)\n",
    "    total_samples = embeddings.shape[0]\n",
    "    top_k = int(total_samples * top_percentage)\n",
    "    print(f\"Selecting top {top_k} samples out of {total_samples} (percentage: {top_percentage * 100}%)...\")\n",
    "\n",
    "    # Find top-k samples with the highest cosine similarity to centroids\n",
    "    max_similarities = np.max(similarities, axis=1)\n",
    "    sorted_indices = np.argsort(max_similarities)[::-1]  # Sort by similarity in descending order\n",
    "    top_indices = sorted_indices[:top_k]  # Select top-k indices\n",
    "\n",
    "    # Generate pseudo-labels for top-k samples\n",
    "    top_1_labels = np.argmax(similarities[top_indices], axis=1)  # Top-1 labels\n",
    "    second_highest_indices = np.argsort(similarities[top_indices], axis=1)[:, -2]  # Top-2 labels\n",
    "    top_2_labels = second_highest_indices\n",
    "\n",
    "    # Return top embeddings and their pseudo-labels\n",
    "    return embeddings[top_indices], top_1_labels, top_2_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Distillation based LWP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationLWP:\n",
    "    def __init__(self, lwp_model, distance_metric='cosine', alpha=0.5, beta=0.5):\n",
    "        \"\"\"\n",
    "        Knowledge Distillation-based LWP Model with top-2 pseudo-labeling.\n",
    "        Args:\n",
    "            distance_metric (str): Distance metric to use for LWP (e.g., cosine).\n",
    "            alpha (float): Weighting factor for distillation loss.\n",
    "            beta (float): Weighting factor for top-2 pseudo-label updates.\n",
    "        \"\"\"\n",
    "        self.old_model = LWP(distance_metric=distance_metric)\n",
    "        self.lwp_model = lwp_model\n",
    "        self.alpha = alpha  # Trade-off between current and old knowledge\n",
    "        self.beta = beta  # Trade-off between top-1 and top-2 pseudo-label prototypes\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        \"\"\"\n",
    "        Fit LWP model with knowledge distillation.\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the current dataset.\n",
    "            labels (np.array): Optional true labels (only for D1).\n",
    "        \"\"\"\n",
    "        # Store the current model as the old model before updating\n",
    "        self.old_model.class_counts = self.lwp_model.class_counts\n",
    "        for i in range(10):\n",
    "            self.old_model.prototypes[i] = self.lwp_model.prototypes[i]\n",
    "        \n",
    "        # Fit the current LWP model to new data\n",
    "        self.lwp_model.fit(features, labels)\n",
    "        # print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAa\")\n",
    "        # print(self.old_model.prototypes.keys())\n",
    "        # print(np.unique(labels))\n",
    "        # print(self.lwp_model.prototypes.keys())\n",
    "        # print(\"Old Model Prototypes after fit:\", self.old_model.prototypes)\n",
    "        print(\"Class Counts after fit in old:\", self.old_model.class_counts)\n",
    "\n",
    "    \n",
    "    def distillation_loss(self, new_features):\n",
    "        \"\"\"\n",
    "        Compute KL Divergence between old model and current model predictions.\n",
    "        Args:\n",
    "            new_features (np.array): Embeddings of the current dataset.\n",
    "        Returns:\n",
    "            float: Knowledge distillation loss.\n",
    "        \"\"\"\n",
    "        if self.old_model is None:\n",
    "            return 0  # No distillation loss for the first dataset\n",
    "        \n",
    "        # Predictions from the old model\n",
    "        old_predictions = self.old_model.predict_proba(new_features)\n",
    "        # print(old_predictions)\n",
    "        # Predictions from the current model\n",
    "        current_predictions = self.lwp_model.predict_proba(new_features)\n",
    "        \n",
    "        # Compute KL divergence\n",
    "        kl_div = np.sum(old_predictions * np.log((old_predictions + 1e-8) / (current_predictions + 1e-8)), axis=1)\n",
    "        return kl_div.mean()\n",
    "\n",
    "    def update_model(self, features):\n",
    "        \"\"\"\n",
    "        Update the LWP model using distillation loss.\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the current dataset.\n",
    "        \"\"\"\n",
    "        if features.size == 0:\n",
    "            print(\"No features available for update. Skipping distillation...\")\n",
    "            return\n",
    "\n",
    "        # Check if prototypes exist\n",
    "        if not self.lwp_model.prototypes:\n",
    "            print(\"No prototypes available. Skipping update...\")\n",
    "            return\n",
    "        # print(features.shape)\n",
    "        # return\n",
    "        centroids = np.vstack([proto for proto in self.lwp_model.prototypes.values()])\n",
    "        top_embeddings, top_1_labels, top_2_labels = select_top_samples(features, centroids)\n",
    "        # print(\"why do i exist:\", np.unique(top_1_labels))\n",
    "        if top_embeddings.shape[0] != top_1_labels.shape[0]:\n",
    "            print(\"Shape mismatch between top_embeddings and top_1_labels. Skipping...\")\n",
    "            return\n",
    "        # Update prototypes using top-1 and top-2 pseudo-labels\n",
    "        iterations = 0\n",
    "        for label in np.unique(top_1_labels):\n",
    "            iterations += 1\n",
    "            top_1_samples = top_embeddings[top_1_labels == label]\n",
    "            top_2_samples = top_embeddings[top_2_labels == label]\n",
    "            # print(\"hi\")\n",
    "            if len(top_1_samples) > 0:\n",
    "                new_proto_top1 = top_1_samples.mean(axis=0)\n",
    "                if len(top_2_samples) > 0:\n",
    "                    new_proto_top2 = top_2_samples.mean(axis=0)\n",
    "                    # print('shapes are')\n",
    "                    # print(new_proto_top1.shape)\n",
    "                    # print(new_proto_top2.shape)\n",
    "                    # Combine top-1 and top-2 updates using the beta factor\n",
    "                    self.lwp_model.prototypes[label] = (\n",
    "                        (1 - self.beta) * new_proto_top1 + self.beta * new_proto_top2\n",
    "                    )\n",
    "                else:\n",
    "                    self.lwp_model.prototypes[label] = new_proto_top1\n",
    "        # print(f\"feature shape (X): {features.shape}\")\n",
    "\n",
    "        if self.old_model:\n",
    "            # Ensure prototype dimensions match features\n",
    "            self.old_model.prototypes = {\n",
    "                label: proto.reshape(-1, features.shape[1])\n",
    "                for label, proto in self.old_model.prototypes.items()\n",
    "        }\n",
    "                    \n",
    "        # Compute distillation loss\n",
    "        # print(self.old_model.prototypes)\n",
    "        distillation_loss = self.distillation_loss(features) if self.old_model.class_counts[0] else 0.0\n",
    "        print(f\"Distillation Loss: {distillation_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        # Adjust prototypes based on distillation loss\n",
    "        \n",
    "        for label, proto in  enumerate(self.lwp_model.prototypes.values()):\n",
    "            # print(\"i'm in love with the shape of X\", label)\n",
    "            if label in self.old_model.prototypes:\n",
    "                self.lwp_model.prototypes[label] = (\n",
    "                    (1 - self.alpha) * proto + self.alpha * self.old_model.prototypes[label]\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Label {label} not found in old model prototypes. Skipping...\")\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Predict pseudo-labels for the given features.\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the data points.\n",
    "        Returns:\n",
    "            np.array: Predicted labels.\n",
    "        \"\"\"\n",
    "        return self.lwp_model.predict(features)\n",
    "    \n",
    "    def predict_proba(self, features):\n",
    "        \"\"\"\n",
    "        Predict probabilities (normalized distances to prototypes).\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the data points.\n",
    "        Returns:\n",
    "            np.array: Predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        distances = []\n",
    "        for feature in features:\n",
    "            dist = {\n",
    "                label: self.lwp_model.distance_fn(feature, proto)\n",
    "                for label, proto in self.lwp_model.prototypes\n",
    "            }\n",
    "            # Convert distances to probabilities\n",
    "            prob = np.exp(-np.array(list(dist.values())))\n",
    "            prob /= prob.sum()\n",
    "            distances.append(prob)\n",
    "        return np.vstack(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a dataset using kNN with top-2 pseudo-labels and knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_on_eval_embeddings(embed_dir, dataset_idx, model, ground_truth=None):\n",
    "    embed_path = os.path.join(embed_dir, f'eval_embeds_{dataset_idx}.pt')\n",
    "    eval_embeddings = torch.load(embed_path, weights_only=False)\n",
    "\n",
    "    print(f\"Evaluating on dataset {dataset_idx}...\")\n",
    "    predicted_labels = model.predict(eval_embeddings)\n",
    "\n",
    "    if ground_truth is not None:\n",
    "        accuracy = accuracy_score(ground_truth, predicted_labels)\n",
    "        report = classification_report(ground_truth, predicted_labels, zero_division=0)\n",
    "        print(f\"Accuracy on eval set {dataset_idx}: {accuracy * 100:.2f}%\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        return {\"accuracy\": accuracy, \"report\": report}\n",
    "    else:\n",
    "        print(f\"Predictions on eval set {dataset_idx}: {predicted_labels[:10]}...\")\n",
    "        return {\"predicted_labels\": predicted_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for embeddings\n",
    "part_one_embed_dir = 'part_1_vit_embeds'\n",
    "part_two_embed_dir = 'part_2_vit_embeds'\n",
    "\n",
    "# Load D1 embeddings and targets\n",
    "train_embed_path = os.path.join(part_one_embed_dir, 'train_embeds_1.pt')\n",
    "eval_embed_path = os.path.join(part_one_embed_dir, 'eval_embeds_1.pt')\n",
    "train_embeddings = torch.load(train_embed_path, weights_only=False)\n",
    "eval_embeddings = torch.load(eval_embed_path, weights_only=False)\n",
    "train_path = os.path.join('dataset', 'part_one_dataset', 'train_data', '1_train_data.tar.pth')\n",
    "data = torch.load(train_path, weights_only=False)\n",
    "targets = data['targets']\n",
    "\n",
    "# Initialize and fit LWP model\n",
    "lwp_model = LWP(distance_metric='cosine')\n",
    "lwp_model.fit(train_embeddings, targets)\n",
    "# print(train_embeddings.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 2 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0000\n",
      "Label 0 not found in old model prototypes. Skipping...\n",
      "Label 1 not found in old model prototypes. Skipping...\n",
      "Label 2 not found in old model prototypes. Skipping...\n",
      "Label 3 not found in old model prototypes. Skipping...\n",
      "Label 4 not found in old model prototypes. Skipping...\n",
      "Label 5 not found in old model prototypes. Skipping...\n",
      "Label 6 not found in old model prototypes. Skipping...\n",
      "Label 7 not found in old model prototypes. Skipping...\n",
      "Label 8 not found in old model prototypes. Skipping...\n",
      "Label 9 not found in old model prototypes. Skipping...\n",
      "Class Counts after fit in old: {0: 489, 1: 503, 2: 403, 3: 498, 4: 549, 5: 436, 6: 519, 7: 585, 8: 498, 9: 520}\n",
      "Dataset 2: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 3 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0001\n",
      "Class Counts after fit in old: {0: 755, 1: 747, 2: 549, 3: 713, 4: 865, 5: 667, 6: 795, 7: 881, 8: 755, 9: 773}\n",
      "Dataset 3: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 4 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0001\n",
      "Class Counts after fit in old: {0: 1004, 1: 1006, 2: 698, 3: 942, 4: 1148, 5: 867, 6: 1062, 7: 1203, 8: 993, 9: 1077}\n",
      "Dataset 4: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 5 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0001\n",
      "Class Counts after fit in old: {0: 1252, 1: 1275, 2: 837, 3: 1165, 4: 1447, 5: 1061, 6: 1314, 7: 1542, 8: 1237, 9: 1370}\n",
      "Dataset 5: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 6 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0000\n",
      "Class Counts after fit in old: {0: 1492, 1: 1495, 2: 992, 3: 1390, 4: 1758, 5: 1307, 6: 1578, 7: 1870, 8: 1467, 9: 1651}\n",
      "Dataset 6: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 7 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0000\n",
      "Class Counts after fit in old: {0: 1738, 1: 1731, 2: 1135, 3: 1600, 4: 2079, 5: 1502, 6: 1826, 7: 2225, 8: 1747, 9: 1917}\n",
      "Dataset 7: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 8 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0000\n",
      "Class Counts after fit in old: {0: 1985, 1: 1980, 2: 1319, 3: 1823, 4: 2369, 5: 1733, 6: 2064, 7: 2541, 8: 1989, 9: 2197}\n",
      "Dataset 8: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 9 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0000\n",
      "Class Counts after fit in old: {0: 2222, 1: 2241, 2: 1465, 3: 2047, 4: 2694, 5: 1934, 6: 2313, 7: 2846, 8: 2256, 9: 2482}\n",
      "Dataset 9: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 10 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0000\n",
      "Class Counts after fit in old: {0: 2467, 1: 2489, 2: 1617, 3: 2291, 4: 2985, 5: 2168, 6: 2554, 7: 3172, 8: 2503, 9: 2754}\n",
      "Dataset 10: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "lwp_model = LWP(distance_metric='cosine')\n",
    "lwp_model.fit(train_embeddings, targets)\n",
    "\n",
    "kd_lwp_model = KnowledgeDistillationLWP(copy.deepcopy(lwp_model),alpha=0.5)\n",
    "\n",
    "# Process D2-D10 with kNN and pseudo-labeling\n",
    "for i in range(2, 11):\n",
    "    dataset_idx = i\n",
    "    k = 5\n",
    "    embed_dir = part_one_embed_dir\n",
    "    top_percentage = 0.5\n",
    "    \"\"\"\n",
    "    Process the dataset using kNN and pseudo-labeling.\n",
    "    \n",
    "    Args:\n",
    "        embed_dir (str): Directory containing embeddings.\n",
    "        dataset_idx (int): Dataset index.\n",
    "        lwp_model (LWP): LWP model instance.\n",
    "        kd_lwp_model (KD-LWP): KD-LWP model instance.\n",
    "        k (int): Number of neighbors for kNN.\n",
    "        top_percentage (float): Percentage of samples to select for pseudo-labeling (0 < top_percentage <= 1).\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset {dataset_idx} with kNN...\")\n",
    "    # Load embeddings\n",
    "    embed_path = os.path.join(embed_dir, f'train_embeds_{dataset_idx}.pt')\n",
    "    embeddings = torch.load(embed_path, weights_only=False)\n",
    "    # print(f\"Loaded embeddings for dataset {dataset_idx}: {embeddings.shape}\")\n",
    "    \n",
    "    if embeddings.size == 0:\n",
    "        print(f\"Dataset {dataset_idx} contains no embeddings. Skipping...\")\n",
    "        break\n",
    "\n",
    "    # Compute centroids (prototypes)\n",
    "    if not lwp_model.prototypes:\n",
    "        print(f\"No prototypes available in LWP model for dataset {dataset_idx}. Skipping...\")\n",
    "        break\n",
    "    # compute prototypes(centroids)\n",
    "    centroids = np.vstack([proto for proto in lwp_model.prototypes.values()])\n",
    "    # Select top 50% samples based on cosine similarity to centroids\n",
    "    top_embeddings, top_1_labels, top_2_labels = select_top_samples(embeddings, centroids, top_percentage=top_percentage)\n",
    "    # Ensure k is not greater than the number of top embeddings\n",
    "    # print(top_embeddings.shape, top_1_labels.shape, top_2_labels.shape)\n",
    "    k = min(k, len(top_embeddings))\n",
    "    # print(f\"Using k={k} for kNN classification (based on top {len(top_embeddings)} embeddings)...\")\n",
    "    \n",
    "    # Train kNN on the top samples\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn.fit(top_embeddings, top_1_labels)\n",
    "\n",
    "    # Use kNN to assign pseudo-labels for all embeddings\n",
    "    all_pseudo_labels = knn.predict(embeddings)\n",
    "    # print(f\"Dataset {dataset_idx}: Assigned pseudo-labels using kNN\")\n",
    "    lwp_model.fit(embeddings, all_pseudo_labels)\n",
    "\n",
    "    # Update prototypes using top-2 pseudo-labels\n",
    "    kd_lwp_model.update_model(embeddings)\n",
    "    # print(\"why so sad: \", kd_lwp_model.lwp_model.prototypes.keys())\n",
    "    kd_lwp_model.fit(embeddings, all_pseudo_labels)\n",
    "    # # Update prototypes (class centroids) with kNN pseudo-labeled samples\n",
    "    # for label in np.unique(all_pseudo_labels):\n",
    "    #     class_embeddings = embeddings[all_pseudo_labels == label]\n",
    "    #     if class_embeddings.size > 0:\n",
    "    #         centroid = class_embeddings.mean(axis=0)\n",
    "    #         lwp_model.prototypes[label] = centroid\n",
    "\n",
    "    print(f\"Dataset {dataset_idx}: Updated prototypes for classes {list(kd_lwp_model.lwp_model.prototypes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cs/t7hq7xj94n13sr47pvj399n00000gn/T/ipykernel_90252/2575434816.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_data = torch.load(eval_labels_path)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on D1 (with ground truth)\n",
    "eval_labels_path = os.path.join('dataset', 'part_one_dataset', 'eval_data', '1_eval_data.tar.pth')\n",
    "eval_data = torch.load(eval_labels_path)\n",
    "eval_ground_truth = eval_data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on dataset 1...\n",
      "Accuracy on eval set 1: 88.12%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90       252\n",
      "           1       0.89      0.96      0.92       217\n",
      "           2       0.98      0.69      0.81       264\n",
      "           3       0.81      0.85      0.83       242\n",
      "           4       0.75      0.91      0.82       257\n",
      "           5       0.86      0.86      0.86       252\n",
      "           6       0.95      0.93      0.94       269\n",
      "           7       0.84      0.88      0.86       233\n",
      "           8       0.95      0.93      0.94       266\n",
      "           9       0.91      0.94      0.93       248\n",
      "\n",
      "    accuracy                           0.88      2500\n",
      "   macro avg       0.89      0.88      0.88      2500\n",
      "weighted avg       0.89      0.88      0.88      2500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8812,\n",
       " 'report': '              precision    recall  f1-score   support\\n\\n           0       0.93      0.88      0.90       252\\n           1       0.89      0.96      0.92       217\\n           2       0.98      0.69      0.81       264\\n           3       0.81      0.85      0.83       242\\n           4       0.75      0.91      0.82       257\\n           5       0.86      0.86      0.86       252\\n           6       0.95      0.93      0.94       269\\n           7       0.84      0.88      0.86       233\\n           8       0.95      0.93      0.94       266\\n           9       0.91      0.94      0.93       248\\n\\n    accuracy                           0.88      2500\\n   macro avg       0.89      0.88      0.88      2500\\nweighted avg       0.89      0.88      0.88      2500\\n'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_on_eval_embeddings(part_one_embed_dir, dataset_idx=1, model=lwp_model, ground_truth=eval_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "   Domain 2  Domain 3  Domain 4  Domain 5  Domain 6  Domain 7  Domain 8  \\\n",
      "0    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "1    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "2    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "3    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "4    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "5    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "6    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "7    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "8    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "9    0.8936    0.8968    0.9068    0.8952    0.9036    0.8936    0.8952   \n",
      "\n",
      "   Domain 9  Domain 10  \n",
      "0     0.894     0.9036  \n",
      "1     0.894     0.9036  \n",
      "2     0.894     0.9036  \n",
      "3     0.894     0.9036  \n",
      "4     0.894     0.9036  \n",
      "5     0.894     0.9036  \n",
      "6     0.894     0.9036  \n",
      "7     0.894     0.9036  \n",
      "8     0.894     0.9036  \n",
      "9     0.894     0.9036  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Function to evaluate multiple datasets and create the desired DataFrame\n",
    "def evaluate_and_create_matrix(embed_dir, model, eval_data_dir, domain_indices, row_indices):\n",
    "    results_matrix = []\n",
    "    \n",
    "    for row_idx in row_indices:  # Loop over rows (e.g., different runs or experiments)\n",
    "        accuracies = []\n",
    "        \n",
    "        for domain_idx in domain_indices:  # Loop over domains (D2-D10)\n",
    "            try:\n",
    "                # Load the evaluation data\n",
    "                eval_data_path = os.path.join(eval_data_dir, f'{domain_idx}_eval_data.tar.pth')\n",
    "                eval_data = torch.load(eval_data_path, weights_only=False)\n",
    "                \n",
    "                # Evaluate the model on the current dataset\n",
    "                result = evaluate_on_eval_embeddings(embed_dir, dataset_idx=domain_idx, model=model, ground_truth=eval_data['targets'])\n",
    "                \n",
    "                # Collect accuracy\n",
    "                accuracies.append(result[\"accuracy\"])\n",
    "            except Exception as e:\n",
    "                # If evaluation fails, append NaN\n",
    "                accuracies.append(None)\n",
    "        \n",
    "        # Add accuracies for this row to the results matrix\n",
    "        results_matrix.append(accuracies)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    domain_columns = [f\"Domain {i}\" for i in domain_indices]\n",
    "    df_results = pd.DataFrame(results_matrix, columns=domain_columns)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Specify directories and indices\n",
    "eval_data_dir = os.path.join('dataset', 'part_one_dataset', 'eval_data')\n",
    "domain_indices = list(range(2, 11))  # Domains D2-D10\n",
    "row_indices = range(10)  # Number of rows (e.g., runs or iterations)\n",
    "\n",
    "# Call the function to get the results DataFrame\n",
    "df_results = evaluate_and_create_matrix(part_one_embed_dir, lwp_model, eval_data_dir, domain_indices, row_indices)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       271\n",
      "           1       0.93      0.96      0.95       261\n",
      "           2       0.97      0.73      0.83       267\n",
      "           3       0.84      0.86      0.85       258\n",
      "           4       0.78      0.92      0.84       236\n",
      "           5       0.82      0.85      0.84       212\n",
      "           6       0.95      0.92      0.94       250\n",
      "           7       0.84      0.89      0.86       252\n",
      "           8       0.96      0.95      0.96       258\n",
      "           9       0.92      0.95      0.94       235\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 89.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       231\n",
      "           1       0.95      0.94      0.94       263\n",
      "           2       0.98      0.73      0.84       275\n",
      "           3       0.86      0.83      0.84       236\n",
      "           4       0.83      0.93      0.88       266\n",
      "           5       0.83      0.88      0.86       232\n",
      "           6       0.94      0.95      0.94       241\n",
      "           7       0.85      0.90      0.88       267\n",
      "           8       0.97      0.94      0.96       241\n",
      "           9       0.89      0.96      0.93       248\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.90      0.90      2500\n",
      "weighted avg       0.90      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 90.68%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       231\n",
      "           1       0.95      0.99      0.97       234\n",
      "           2       0.97      0.79      0.87       247\n",
      "           3       0.83      0.85      0.84       241\n",
      "           4       0.84      0.94      0.89       261\n",
      "           5       0.86      0.83      0.84       273\n",
      "           6       0.96      0.97      0.96       251\n",
      "           7       0.87      0.90      0.89       263\n",
      "           8       0.96      0.95      0.95       251\n",
      "           9       0.94      0.96      0.95       248\n",
      "\n",
      "    accuracy                           0.91      2500\n",
      "   macro avg       0.91      0.91      0.91      2500\n",
      "weighted avg       0.91      0.91      0.91      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94       256\n",
      "           1       0.96      0.96      0.96       265\n",
      "           2       0.96      0.67      0.79       239\n",
      "           3       0.82      0.86      0.84       251\n",
      "           4       0.72      0.94      0.82       227\n",
      "           5       0.88      0.87      0.87       246\n",
      "           6       0.96      0.92      0.94       256\n",
      "           7       0.85      0.88      0.87       261\n",
      "           8       0.96      0.96      0.96       249\n",
      "           9       0.93      0.98      0.95       250\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       245\n",
      "           1       0.95      0.96      0.96       276\n",
      "           2       0.98      0.75      0.85       235\n",
      "           3       0.83      0.86      0.84       226\n",
      "           4       0.84      0.98      0.90       257\n",
      "           5       0.88      0.84      0.86       271\n",
      "           6       0.94      0.94      0.94       235\n",
      "           7       0.86      0.90      0.88       257\n",
      "           8       0.95      0.95      0.95       234\n",
      "           9       0.92      0.96      0.94       264\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 89.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       245\n",
      "           1       0.95      0.95      0.95       239\n",
      "           2       0.97      0.72      0.82       251\n",
      "           3       0.83      0.85      0.84       247\n",
      "           4       0.79      0.92      0.85       268\n",
      "           5       0.88      0.87      0.87       272\n",
      "           6       0.92      0.95      0.93       266\n",
      "           7       0.84      0.86      0.85       232\n",
      "           8       0.97      0.94      0.95       238\n",
      "           9       0.93      0.96      0.95       242\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 89.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       267\n",
      "           1       0.94      0.97      0.95       238\n",
      "           2       0.98      0.70      0.81       238\n",
      "           3       0.83      0.83      0.83       248\n",
      "           4       0.75      0.91      0.82       247\n",
      "           5       0.88      0.90      0.89       251\n",
      "           6       0.95      0.93      0.94       253\n",
      "           7       0.85      0.88      0.86       257\n",
      "           8       0.96      0.94      0.95       264\n",
      "           9       0.93      0.96      0.95       237\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.90      0.89      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 89.40%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92       268\n",
      "           1       0.95      0.95      0.95       261\n",
      "           2       0.98      0.69      0.81       236\n",
      "           3       0.83      0.85      0.84       253\n",
      "           4       0.79      0.96      0.87       254\n",
      "           5       0.85      0.85      0.85       270\n",
      "           6       0.97      0.93      0.95       250\n",
      "           7       0.85      0.89      0.87       246\n",
      "           8       0.94      0.96      0.95       218\n",
      "           9       0.90      0.95      0.92       244\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.90      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 90.36%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94       244\n",
      "           1       0.92      0.98      0.95       247\n",
      "           2       0.97      0.70      0.81       234\n",
      "           3       0.88      0.87      0.88       270\n",
      "           4       0.76      0.94      0.84       219\n",
      "           5       0.85      0.88      0.87       251\n",
      "           6       0.96      0.94      0.95       252\n",
      "           7       0.85      0.88      0.87       241\n",
      "           8       0.99      0.97      0.98       284\n",
      "           9       0.92      0.94      0.93       258\n",
      "\n",
      "    accuracy                           0.90      2500\n",
      "   macro avg       0.91      0.90      0.90      2500\n",
      "weighted avg       0.91      0.90      0.90      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on D2-D10\n",
    "for i in range(2, 11):\n",
    "    t = torch.load(os.path.join('dataset', 'part_one_dataset', 'eval_data', f'{i}_eval_data.tar.pth') ,weights_only=False)\n",
    "    evaluate_on_eval_embeddings(part_one_embed_dir, dataset_idx=i, model=lwp_model, ground_truth = t['targets']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset D11 from part_2_vit_embeds/train_embeds_1.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0002\n",
      "Pseudo-labels for eval set of D11: [4 4 0 0 1 7 3 8 6 6]\n",
      "Processing dataset D12 from part_2_vit_embeds/train_embeds_2.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0005\n",
      "Pseudo-labels for eval set of D12: [4 3 8 8 9 7 3 0 4 4]\n",
      "Processing dataset D13 from part_2_vit_embeds/train_embeds_3.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0002\n",
      "Pseudo-labels for eval set of D13: [4 4 8 8 1 4 3 0 7 2]\n",
      "Processing dataset D14 from part_2_vit_embeds/train_embeds_4.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0001\n",
      "Pseudo-labels for eval set of D14: [4 4 8 8 1 7 3 0 6 2]\n",
      "Processing dataset D15 from part_2_vit_embeds/train_embeds_5.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0001\n",
      "Pseudo-labels for eval set of D15: [4 4 8 8 1 7 3 0 6 2]\n",
      "Processing dataset D16 from part_2_vit_embeds/train_embeds_6.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0002\n",
      "Pseudo-labels for eval set of D16: [3 3 8 8 1 7 3 0 6 2]\n",
      "Processing dataset D17 from part_2_vit_embeds/train_embeds_7.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0002\n",
      "Pseudo-labels for eval set of D17: [4 4 8 8 1 7 3 6 2 2]\n",
      "Processing dataset D18 from part_2_vit_embeds/train_embeds_8.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0002\n",
      "Pseudo-labels for eval set of D18: [4 4 8 8 1 7 3 0 6 2]\n",
      "Processing dataset D19 from part_2_vit_embeds/train_embeds_9.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0008\n",
      "Pseudo-labels for eval set of D19: [4 4 8 8 1 7 3 0 4 0]\n",
      "Processing dataset D20 from part_2_vit_embeds/train_embeds_10.pt...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Distillation Loss: 0.0002\n",
      "Pseudo-labels for eval set of D20: [4 4 8 8 1 7 3 0 6 2]\n"
     ]
    }
   ],
   "source": [
    "# Process D11-D20 (unlabeled datasets) with knowledge distillation\n",
    "for i in range(11, 21):\n",
    "    train_path = os.path.join(part_two_embed_dir, f'train_embeds_{i-10}.pt')\n",
    "    print(f\"Processing dataset D{i} from {train_path}...\")\n",
    "    \n",
    "    # Load train embeddings\n",
    "    train_embeddings = torch.load(train_path, weights_only=False)\n",
    "    \n",
    "    # Perform knowledge distillation-based learning\n",
    "    kd_lwp_model.update_model(train_embeddings)\n",
    "    \n",
    "    #FIXME: Uncomment this line to update the model with the new data, where to get the pseudo labels from?\n",
    "    # kd_lwp_model.fit(train_embeddings)\n",
    "    \n",
    "    # Evaluate or predict on eval set if needed\n",
    "    eval_path = os.path.join(part_two_embed_dir, f'eval_embeds_{i-10}.pt')\n",
    "    eval_embeddings = torch.load(eval_path, weights_only=False)\n",
    "    predictions = kd_lwp_model.predict(eval_embeddings)\n",
    "    print(f\"Pseudo-labels for eval set of D{i}: {predictions[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on dataset 1...\n",
      "Accuracy on eval set 1: 81.52%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85       238\n",
      "           1       0.86      0.92      0.89       262\n",
      "           2       0.93      0.66      0.77       256\n",
      "           3       0.61      0.78      0.69       255\n",
      "           4       0.77      0.84      0.80       268\n",
      "           5       0.72      0.79      0.75       246\n",
      "           6       0.83      0.83      0.83       225\n",
      "           7       0.88      0.78      0.83       259\n",
      "           8       0.93      0.84      0.88       236\n",
      "           9       0.92      0.85      0.88       255\n",
      "\n",
      "    accuracy                           0.82      2500\n",
      "   macro avg       0.83      0.82      0.82      2500\n",
      "weighted avg       0.83      0.82      0.82      2500\n",
      "\n",
      "Evaluating on dataset 2...\n",
      "Accuracy on eval set 2: 67.32%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.88      0.62       238\n",
      "           1       0.95      0.40      0.57       262\n",
      "           2       0.95      0.30      0.46       256\n",
      "           3       0.62      0.51      0.56       255\n",
      "           4       0.71      0.75      0.73       268\n",
      "           5       0.64      0.81      0.72       246\n",
      "           6       0.63      0.89      0.74       225\n",
      "           7       0.81      0.73      0.77       259\n",
      "           8       0.76      0.84      0.80       236\n",
      "           9       0.68      0.67      0.67       255\n",
      "\n",
      "    accuracy                           0.67      2500\n",
      "   macro avg       0.72      0.68      0.66      2500\n",
      "weighted avg       0.73      0.67      0.66      2500\n",
      "\n",
      "Evaluating on dataset 3...\n",
      "Accuracy on eval set 3: 84.64%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       238\n",
      "           1       0.91      0.90      0.90       262\n",
      "           2       0.96      0.62      0.76       256\n",
      "           3       0.76      0.73      0.75       255\n",
      "           4       0.76      0.89      0.82       268\n",
      "           5       0.81      0.84      0.83       246\n",
      "           6       0.86      0.91      0.88       225\n",
      "           7       0.88      0.88      0.88       259\n",
      "           8       0.89      0.92      0.90       236\n",
      "           9       0.84      0.93      0.88       255\n",
      "\n",
      "    accuracy                           0.85      2500\n",
      "   macro avg       0.85      0.85      0.85      2500\n",
      "weighted avg       0.85      0.85      0.84      2500\n",
      "\n",
      "Evaluating on dataset 4...\n",
      "Accuracy on eval set 4: 88.24%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88       238\n",
      "           1       0.95      0.95      0.95       262\n",
      "           2       0.96      0.73      0.83       256\n",
      "           3       0.81      0.83      0.82       255\n",
      "           4       0.79      0.90      0.84       268\n",
      "           5       0.82      0.84      0.83       246\n",
      "           6       0.91      0.93      0.92       225\n",
      "           7       0.89      0.87      0.88       259\n",
      "           8       0.95      0.95      0.95       236\n",
      "           9       0.92      0.95      0.93       255\n",
      "\n",
      "    accuracy                           0.88      2500\n",
      "   macro avg       0.89      0.88      0.88      2500\n",
      "weighted avg       0.89      0.88      0.88      2500\n",
      "\n",
      "Evaluating on dataset 5...\n",
      "Accuracy on eval set 5: 89.20%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89       238\n",
      "           1       0.96      0.95      0.96       262\n",
      "           2       0.97      0.77      0.86       256\n",
      "           3       0.84      0.82      0.83       255\n",
      "           4       0.82      0.91      0.86       268\n",
      "           5       0.84      0.85      0.84       246\n",
      "           6       0.95      0.92      0.93       225\n",
      "           7       0.86      0.91      0.88       259\n",
      "           8       0.94      0.93      0.94       236\n",
      "           9       0.90      0.96      0.93       255\n",
      "\n",
      "    accuracy                           0.89      2500\n",
      "   macro avg       0.90      0.89      0.89      2500\n",
      "weighted avg       0.89      0.89      0.89      2500\n",
      "\n",
      "Evaluating on dataset 6...\n",
      "Accuracy on eval set 6: 82.04%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.88      0.81       238\n",
      "           1       0.86      0.95      0.91       262\n",
      "           2       0.98      0.57      0.72       256\n",
      "           3       0.69      0.74      0.71       255\n",
      "           4       0.80      0.80      0.80       268\n",
      "           5       0.84      0.79      0.81       246\n",
      "           6       0.73      0.91      0.81       225\n",
      "           7       0.84      0.87      0.85       259\n",
      "           8       0.93      0.80      0.86       236\n",
      "           9       0.89      0.91      0.90       255\n",
      "\n",
      "    accuracy                           0.82      2500\n",
      "   macro avg       0.83      0.82      0.82      2500\n",
      "weighted avg       0.83      0.82      0.82      2500\n",
      "\n",
      "Evaluating on dataset 7...\n",
      "Accuracy on eval set 7: 83.60%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.83       238\n",
      "           1       0.88      0.94      0.91       262\n",
      "           2       0.92      0.72      0.81       256\n",
      "           3       0.62      0.83      0.71       255\n",
      "           4       0.76      0.88      0.82       268\n",
      "           5       0.83      0.76      0.80       246\n",
      "           6       0.86      0.88      0.87       225\n",
      "           7       0.93      0.78      0.85       259\n",
      "           8       0.95      0.86      0.90       236\n",
      "           9       0.92      0.88      0.90       255\n",
      "\n",
      "    accuracy                           0.84      2500\n",
      "   macro avg       0.85      0.84      0.84      2500\n",
      "weighted avg       0.85      0.84      0.84      2500\n",
      "\n",
      "Evaluating on dataset 8...\n",
      "Accuracy on eval set 8: 82.48%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83       238\n",
      "           1       0.95      0.88      0.91       262\n",
      "           2       0.98      0.59      0.74       256\n",
      "           3       0.74      0.71      0.73       255\n",
      "           4       0.71      0.90      0.79       268\n",
      "           5       0.78      0.78      0.78       246\n",
      "           6       0.73      0.93      0.82       225\n",
      "           7       0.88      0.84      0.86       259\n",
      "           8       0.88      0.89      0.89       236\n",
      "           9       0.86      0.93      0.90       255\n",
      "\n",
      "    accuracy                           0.82      2500\n",
      "   macro avg       0.84      0.83      0.82      2500\n",
      "weighted avg       0.84      0.82      0.82      2500\n",
      "\n",
      "Evaluating on dataset 9...\n",
      "Accuracy on eval set 9: 68.96%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.75      0.70       238\n",
      "           1       0.80      0.69      0.74       262\n",
      "           2       0.98      0.35      0.52       256\n",
      "           3       0.58      0.55      0.56       255\n",
      "           4       0.83      0.61      0.70       268\n",
      "           5       0.73      0.70      0.71       246\n",
      "           6       0.86      0.57      0.69       225\n",
      "           7       0.68      0.79      0.73       259\n",
      "           8       0.64      0.93      0.76       236\n",
      "           9       0.56      0.96      0.70       255\n",
      "\n",
      "    accuracy                           0.69      2500\n",
      "   macro avg       0.73      0.69      0.68      2500\n",
      "weighted avg       0.73      0.69      0.68      2500\n",
      "\n",
      "Evaluating on dataset 10...\n",
      "Accuracy on eval set 10: 86.60%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       238\n",
      "           1       0.96      0.90      0.93       262\n",
      "           2       0.97      0.68      0.80       256\n",
      "           3       0.81      0.84      0.82       255\n",
      "           4       0.82      0.83      0.83       268\n",
      "           5       0.89      0.82      0.85       246\n",
      "           6       0.81      0.96      0.88       225\n",
      "           7       0.83      0.90      0.86       259\n",
      "           8       0.94      0.90      0.92       236\n",
      "           9       0.84      0.96      0.90       255\n",
      "\n",
      "    accuracy                           0.87      2500\n",
      "   macro avg       0.87      0.87      0.87      2500\n",
      "weighted avg       0.87      0.87      0.86      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on D11-D20 (unlabeled datasets)\n",
    "for i in range(11, 21):\n",
    "    t = torch.load(os.path.join('dataset', 'part_two_dataset', 'eval_data', f'{i-10}_eval_data.tar.pth') ,weights_only=False)\n",
    "    evaluate_on_eval_embeddings(part_two_embed_dir, dataset_idx=i-10, model=lwp_model, ground_truth=t['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "771",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
