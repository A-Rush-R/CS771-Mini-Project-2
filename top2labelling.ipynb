{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import models, transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('dataset', 'part_one_dataset', 'train_data')\n",
    "eval_dir = os.path.join('dataset', 'part_one_dataset', 'eval_data')\n",
    "train_path = os.path.join(train_dir, '1_train_data.tar.pth')\n",
    "eval_path = os.path.join(eval_dir, '1_eval_data.tar.pth')\n",
    "\n",
    "t = torch.load(train_path, weights_only = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic LWP Model using Distance Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances, manhattan_distances\n",
    "\n",
    "class LWP:\n",
    "    \"\"\"Learning Vector Prototypes with configurable distance function\"\"\"\n",
    "    \n",
    "    DISTANCE_FUNCTIONS = {\n",
    "        'euclidean': lambda x, y: np.linalg.norm(x - y),\n",
    "        'cosine': lambda x, y: cosine_distances(x.reshape(1, -1), y.reshape(1, -1))[0][0],\n",
    "        'manhattan': lambda x, y: manhattan_distances(x.reshape(1, -1), y.reshape(1, -1))[0][0],\n",
    "        'minkowski': lambda x, y, p=2: np.power(np.sum(np.power(np.abs(x - y), p)), 1/p)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, distance_metric='euclidean', **distance_params):\n",
    "        \"\"\"\n",
    "            distance_params (dict): Additional parameters for the distance function\n",
    "        \"\"\"\n",
    "        self.prototypes = {}\n",
    "        self.class_counts = {i: 0 for i in range(10)}\n",
    "        \n",
    "        if callable(distance_metric):\n",
    "            self.distance_fn = distance_metric\n",
    "        elif distance_metric in self.DISTANCE_FUNCTIONS:\n",
    "            if distance_metric == 'minkowski':\n",
    "                p = distance_params.get('p', 2)\n",
    "                self.distance_fn = lambda x, y: self.DISTANCE_FUNCTIONS[distance_metric](x, y, p)\n",
    "            else:\n",
    "                self.distance_fn = self.DISTANCE_FUNCTIONS[distance_metric]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {distance_metric}. \" \n",
    "                           f\"Available metrics: {list(self.DISTANCE_FUNCTIONS.keys())}\")\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            samples = features[labels == label]\n",
    "            num_samples = len(samples)\n",
    "            \n",
    "            if label not in self.prototypes:  # Original condition was: if label not in self.prototypes\n",
    "                self.prototypes[label] = samples.mean(axis=0)\n",
    "                self.class_counts[label] = len(samples)\n",
    "            else:\n",
    "                self.class_counts[label] += len(samples)\n",
    "                self.prototypes[label] = (\n",
    "                    (self.class_counts[label] - num_samples) / self.class_counts[label] * self.prototypes[label] +\n",
    "                    num_samples / self.class_counts[label] * samples.mean(axis=0)\n",
    "                )\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "    def predict(self, features):\n",
    "        preds = []\n",
    "        for feature in features:\n",
    "            distances = {\n",
    "                label: self.distance_fn(feature, proto)\n",
    "                for label, proto in self.prototypes.items()\n",
    "            }\n",
    "            preds.append(min(distances, key=distances.get))\n",
    "        return np.array(preds)\n",
    "    \n",
    "    def predict_proba(self, features):\n",
    "        \"\"\"\n",
    "        Predict probabilities (normalized distances to prototypes).\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the data points.\n",
    "        Returns:\n",
    "            np.array: Probabilities for each class.\n",
    "        \"\"\"\n",
    "        prob_list = []\n",
    "        for feature in features:\n",
    "            distances = {\n",
    "                label: self.distance_fn(feature, proto) for label, proto in self.prototypes.items()\n",
    "            }\n",
    "            # Convert distances to probabilities\n",
    "            prob = np.exp(-np.array(list(distances.values())))  # Exponential of negative distances\n",
    "            prob /= prob.sum()  # Normalize to sum to 1\n",
    "            prob_list.append(prob)\n",
    "        return np.vstack(prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 3072)\n"
     ]
    }
   ],
   "source": [
    "data, targets = t['data'], t['targets'] \n",
    "data=data.reshape(data.shape[0], -1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, targets1= t['data'], t['targets']\n",
    "data1=data1.reshape(data1.shape[0], -1)\n",
    "data1=normalize(data1)\n",
    "dataloader=DataLoader(data1, batch_size=32, shuffle=False)\n",
    "lwp_model = LWP(distance_metric='cosine')  # LWP model with cosine distance\n",
    "lwp_model.fit(data1, targets1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypes after fit: {0: array([0.01729068, 0.01912501, 0.02102479, ..., 0.01516719, 0.0159201 ,\n",
      "       0.01601679]), 1: array([0.01837207, 0.01810205, 0.01747006, ..., 0.01770059, 0.01728502,\n",
      "       0.01624851]), 2: array([0.0164336 , 0.01742426, 0.01454287, ..., 0.01747445, 0.01748331,\n",
      "       0.01434357]), 3: array([0.01795165, 0.0166975 , 0.01529295, ..., 0.01780526, 0.01648547,\n",
      "       0.01536131]), 4: array([0.01489708, 0.01576391, 0.01352558, ..., 0.0190007 , 0.01918124,\n",
      "       0.01478338]), 5: array([0.01479913, 0.01467359, 0.01309324, ..., 0.01706052, 0.01606419,\n",
      "       0.01416187]), 6: array([0.0163735 , 0.01605917, 0.01306188, ..., 0.01921913, 0.01844561,\n",
      "       0.01541324]), 7: array([0.01802458, 0.01884868, 0.01836053, ..., 0.01945586, 0.01867545,\n",
      "       0.01490231]), 8: array([0.01797723, 0.02032782, 0.02269468, ..., 0.01198278, 0.01380034,\n",
      "       0.01474392]), 9: array([0.02226604, 0.02317505, 0.02398837, ..., 0.01793398, 0.01759059,\n",
      "       0.0167393 ])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prototypes after fit:\", lwp_model.prototypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity and select top samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_samples(embeddings, centroids, top_percentage=0.5):\n",
    "    \"\"\"\n",
    "    Select top-k% samples with highest cosine similarity to centroids\n",
    "    and include top-2 pseudo-labels.\n",
    "    Args:\n",
    "        embeddings (np.array): The data embeddings.\n",
    "        centroids (np.array): Prototypes or centroids for each class.\n",
    "        top_percentage (float): Percentage of top samples to select (0 < top_percentage <= 1).\n",
    "    Returns:\n",
    "        tuple: Top-k% embeddings, top-1 pseudo-labels, and top-2 pseudo-labels.\n",
    "    \"\"\"\n",
    "    if not (0 < top_percentage <= 1):\n",
    "        raise ValueError(\"top_percentage must be between 0 and 1.\")\n",
    "\n",
    "    # Compute similarity scores\n",
    "    similarities = cosine_similarity(embeddings, centroids)\n",
    "\n",
    "    # Compute the number of samples to select (50% of total embeddings)\n",
    "    total_samples = embeddings.shape[0]\n",
    "    top_k = int(total_samples * top_percentage)\n",
    "    print(f\"Selecting top {top_k} samples out of {total_samples} (percentage: {top_percentage * 100}%)...\")\n",
    "\n",
    "    # Find top-k samples with the highest cosine similarity to centroids\n",
    "    max_similarities = np.max(similarities, axis=1)\n",
    "    sorted_indices = np.argsort(max_similarities)[::-1]  # Sort by similarity in descending order\n",
    "    top_indices = sorted_indices[:top_k]  # Select top-k indices\n",
    "\n",
    "    # Generate pseudo-labels for top-k samples\n",
    "    top_1_labels = np.argmax(similarities[top_indices], axis=1)  # Top-1 labels\n",
    "    second_highest_indices = np.argsort(similarities[top_indices], axis=1)[:, -2]  # Top-2 labels\n",
    "    top_2_labels = second_highest_indices\n",
    "\n",
    "    # Return top embeddings and their pseudo-labels\n",
    "    return embeddings[top_indices], top_1_labels, top_2_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Distillation based LWP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationLWP:\n",
    "    def __init__(self, distance_metric='cosine', alpha=0.5, beta=0.5):\n",
    "        \"\"\"\n",
    "        Knowledge Distillation-based LWP Model with top-2 pseudo-labeling.\n",
    "        Args:\n",
    "            distance_metric (str): Distance metric to use for LWP (e.g., cosine).\n",
    "            alpha (float): Weighting factor for distillation loss.\n",
    "            beta (float): Weighting factor for top-2 pseudo-label updates.\n",
    "        \"\"\"\n",
    "        self.lwp_model = LWP(distance_metric=distance_metric)\n",
    "        self.old_model = None  # Placeholder for storing old model\n",
    "        self.alpha = alpha  # Trade-off between current and old knowledge\n",
    "        self.beta = beta  # Trade-off between top-1 and top-2 pseudo-label prototypes\n",
    "\n",
    "    def fit(self, features, labels=None):\n",
    "        \"\"\"\n",
    "        Fit LWP model with knowledge distillation.\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the current dataset.\n",
    "            labels (np.array): Optional true labels (only for D1).\n",
    "        \"\"\"\n",
    "        # Store the current model as the old model before updating\n",
    "        if self.old_model is None:\n",
    "            self.old_model = LWP(distance_metric='cosine')\n",
    "            self.old_model.prototypes = self.lwp_model.prototypes.copy()\n",
    "            self.old_model.class_counts = self.lwp_model.class_counts.copy()\n",
    "        \n",
    "        # Fit the current LWP model to new data\n",
    "        self.lwp_model.fit(features, labels)\n",
    "    \n",
    "    def distillation_loss(self, new_features):\n",
    "        \"\"\"\n",
    "        Compute KL Divergence between old model and current model predictions.\n",
    "        Args:\n",
    "            new_features (np.array): Embeddings of the current dataset.\n",
    "        Returns:\n",
    "            float: Knowledge distillation loss.\n",
    "        \"\"\"\n",
    "        if self.old_model is None:\n",
    "            return 0  # No distillation loss for the first dataset\n",
    "        \n",
    "        # Predictions from the old model\n",
    "        old_predictions = self.old_model.predict_proba(new_features)\n",
    "        # Predictions from the current model\n",
    "        current_predictions = self.lwp_model.predict_proba(new_features)\n",
    "        \n",
    "        # Compute KL divergence\n",
    "        kl_div = np.sum(old_predictions * np.log((old_predictions + 1e-8) / (current_predictions + 1e-8)), axis=1)\n",
    "        return kl_div.mean()\n",
    "\n",
    "    def update_model(self, features):\n",
    "        \"\"\"\n",
    "        Update the LWP model using distillation loss.\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the current dataset.\n",
    "        \"\"\"\n",
    "        if features.size == 0:\n",
    "            print(\"No features available for update. Skipping distillation...\")\n",
    "            return\n",
    "\n",
    "        # Check if prototypes exist\n",
    "        if not self.lwp_model.prototypes:\n",
    "            print(\"No prototypes available. Skipping update...\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Retrieve prototypes and compute pseudo-labels\n",
    "            centroids = np.vstack([proto for _, proto in sorted(self.lwp_model.prototypes.items())])\n",
    "        except ValueError as e:\n",
    "            print(f\"Error constructing centroids: {e}. Skipping update...\")\n",
    "            return\n",
    "        # Retrieve prototypes and compute pseudo-labels\n",
    "        centroids = np.vstack([proto for _, proto in sorted(self.lwp_model.prototypes.items())])\n",
    "        top_embeddings, top_1_labels, top_2_labels = select_top_samples(features, centroids)\n",
    "        if top_embeddings.shape[0] != top_1_labels.shape[0]:\n",
    "            print(\"Shape mismatch between top_embeddings and top_1_labels. Skipping...\")\n",
    "            return\n",
    "        # Update prototypes using top-1 and top-2 pseudo-labels\n",
    "        for label in np.unique(top_1_labels):\n",
    "            top_1_samples = top_embeddings[top_1_labels == label]\n",
    "            top_2_samples = top_embeddings[top_2_labels == label]\n",
    "            \n",
    "            if len(top_1_samples) > 0:\n",
    "                new_proto_top1 = top_1_samples.mean(axis=0)\n",
    "                if len(top_2_samples) > 0:\n",
    "                    new_proto_top2 = top_2_samples.mean(axis=0)\n",
    "                    # Combine top-1 and top-2 updates using the beta factor\n",
    "                    self.lwp_model.prototypes[label] = (\n",
    "                        (1 - self.beta) * new_proto_top1 + self.beta * new_proto_top2\n",
    "                    )\n",
    "                else:\n",
    "                    self.lwp_model.prototypes[label] = new_proto_top1\n",
    "        print(f\"feature shape (X): {features.shape}\")\n",
    "        if self.old_model:\n",
    "            for label, proto in self.old_model.prototypes.items():\n",
    "                print(f\"Prototype for class {label}: Shape = {proto.shape}\")\n",
    "\n",
    "        \n",
    "        # Align embedding dimensions\n",
    "        features = features.reshape(features.shape[0], -1)\n",
    "        \n",
    "        if self.old_model:\n",
    "            # Ensure prototype dimensions match features\n",
    "            self.old_model.prototypes = {\n",
    "                label: proto.reshape(-1, features.shape[1])\n",
    "                for label, proto in self.old_model.prototypes.items()\n",
    "        }\n",
    "                    \n",
    "        # Compute distillation loss\n",
    "        distillation_loss = self.distillation_loss(features) if self.old_model else 0.0\n",
    "        print(f\"Distillation Loss: {distillation_loss:.4f}\")\n",
    "\n",
    "        \n",
    "        # Adjust prototypes based on distillation loss\n",
    "        for label, proto in self.lwp_model.prototypes.items():\n",
    "            if label in self.old_model.prototypes:\n",
    "                self.lwp_model.prototypes[label] = (\n",
    "                    (1 - self.alpha) * proto + self.alpha * self.old_model.prototypes[label]\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Label {label} not found in old model prototypes. Skipping...\")\n",
    "\n",
    "    def predict(self, features):\n",
    "        \"\"\"\n",
    "        Predict pseudo-labels for the given features.\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the data points.\n",
    "        Returns:\n",
    "            np.array: Predicted labels.\n",
    "        \"\"\"\n",
    "        return self.lwp_model.predict(features)\n",
    "    \n",
    "    def predict_proba(self, features):\n",
    "        \"\"\"\n",
    "        Predict probabilities (normalized distances to prototypes).\n",
    "        Args:\n",
    "            features (np.array): Embeddings of the data points.\n",
    "        Returns:\n",
    "            np.array: Predicted probabilities for each class.\n",
    "        \"\"\"\n",
    "        distances = []\n",
    "        for feature in features:\n",
    "            dist = {\n",
    "                label: self.lwp_model.distance_fn(feature, proto)\n",
    "                for label, proto in self.lwp_model.prototypes.items()\n",
    "            }\n",
    "            # Convert distances to probabilities\n",
    "            prob = np.exp(-np.array(list(dist.values())))\n",
    "            prob /= prob.sum()\n",
    "            distances.append(prob)\n",
    "        return np.vstack(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process a dataset using kNN with top-2 pseudo-labels and knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Knowledge Distillation LWP model\n",
    "kd_lwp_model = KnowledgeDistillationLWP(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_knn(embed_dir, dataset_idx, lwp_model, kd_lwp_model, k=5, top_percentage=0.5):\n",
    "    \"\"\"\n",
    "    Process the dataset using kNN and pseudo-labeling.\n",
    "    \n",
    "    Args:\n",
    "        embed_dir (str): Directory containing embeddings.\n",
    "        dataset_idx (int): Dataset index.\n",
    "        lwp_model (LWP): LWP model instance.\n",
    "        kd_lwp_model (KD-LWP): KD-LWP model instance.\n",
    "        k (int): Number of neighbors for kNN.\n",
    "        top_percentage (float): Percentage of samples to select for pseudo-labeling (0 < top_percentage <= 1).\n",
    "    \"\"\"\n",
    "    print(f\"Processing dataset {dataset_idx} with kNN...\")\n",
    "    # Load embeddings\n",
    "    embed_path = os.path.join(embed_dir, f'train_embeds_{dataset_idx}.pt')\n",
    "    embeddings = torch.load(embed_path)\n",
    "    if embeddings.size == 0:\n",
    "        print(f\"Dataset {dataset_idx} contains no embeddings. Skipping...\")\n",
    "        return\n",
    "\n",
    "    # Compute centroids (prototypes)\n",
    "    if not lwp_model.prototypes:\n",
    "        print(f\"No prototypes available in LWP model for dataset {dataset_idx}. Skipping...\")\n",
    "        return\n",
    "    # compute prototypes(centroids)\n",
    "    centroids = np.vstack([proto for _, proto in sorted(lwp_model.prototypes.items())])\n",
    "    # Select top 50% samples based on cosine similarity to centroids\n",
    "    top_embeddings, top_1_labels, top_2_labels = select_top_samples(embeddings, centroids, top_percentage=top_percentage)\n",
    "    # Ensure k is not greater than the number of top embeddings\n",
    "    k = min(k, len(top_embeddings))\n",
    "    print(f\"Using k={k} for kNN classification (based on top {len(top_embeddings)} embeddings)...\")\n",
    "    \n",
    "    \n",
    "    # Train kNN on the top samples\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn.fit(top_embeddings, top_1_labels)\n",
    "\n",
    "    # Use kNN to assign pseudo-labels for all embeddings\n",
    "    all_pseudo_labels = knn.predict(embeddings)\n",
    "    print(f\"Dataset {dataset_idx}: Assigned pseudo-labels using kNN\")\n",
    "    lwp_model.fit(embeddings, all_pseudo_labels)\n",
    "    # Update prototypes using top-2 pseudo-labels\n",
    "    kd_lwp_model.update_model(embeddings)\n",
    "    kd_lwp_model.fit(embeddings)\n",
    "    # # Update prototypes (class centroids) with kNN pseudo-labeled samples\n",
    "    # for label in np.unique(all_pseudo_labels):\n",
    "    #     class_embeddings = embeddings[all_pseudo_labels == label]\n",
    "    #     if class_embeddings.size > 0:\n",
    "    #         centroid = class_embeddings.mean(axis=0)\n",
    "    #         lwp_model.prototypes[label] = centroid\n",
    "\n",
    "    print(f\"Dataset {dataset_idx}: Updated prototypes for classes {list(kd_lwp_model.lwp_model.prototypes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_on_eval_embeddings(embed_dir, dataset_idx, model, ground_truth=None):\n",
    "    embed_path = os.path.join(embed_dir, f'eval_embeds_{dataset_idx}.pt')\n",
    "    eval_embeddings = torch.load(embed_path)\n",
    "\n",
    "    print(f\"Evaluating on dataset {dataset_idx}...\")\n",
    "    predicted_labels = model.predict(eval_embeddings)\n",
    "\n",
    "    if ground_truth is not None:\n",
    "        accuracy = accuracy_score(ground_truth, predicted_labels)\n",
    "        report = classification_report(ground_truth, predicted_labels, zero_division=0)\n",
    "        print(f\"Accuracy on eval set {dataset_idx}: {accuracy * 100:.2f}%\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        return {\"accuracy\": accuracy, \"report\": report}\n",
    "    else:\n",
    "        print(f\"Predictions on eval set {dataset_idx}: {predicted_labels[:10]}...\")\n",
    "        return {\"predicted_labels\": predicted_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3655030940.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_embeddings = torch.load(train_embed_path)\n",
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3655030940.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(eval_embed_path)\n",
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3655030940.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(train_path)\n"
     ]
    }
   ],
   "source": [
    "# Directories for embeddings\n",
    "part_one_embed_dir = 'part_1_vit_embeds'\n",
    "part_two_embed_dir = 'part_2_vit_embeds'\n",
    "\n",
    "# Load D1 embeddings and targets\n",
    "train_embed_path = os.path.join(part_one_embed_dir, 'train_embeds_1.pt')\n",
    "eval_embed_path = os.path.join(part_one_embed_dir, 'eval_embeds_1.pt')\n",
    "train_embeddings = torch.load(train_embed_path)\n",
    "eval_embeddings = torch.load(eval_embed_path)\n",
    "train_path = os.path.join('dataset', 'part_one_dataset', 'train_data', '1_train_data.tar.pth')\n",
    "data = torch.load(train_path)\n",
    "targets = data['targets']\n",
    "\n",
    "# Initialize and fit LWP model\n",
    "lwp_model = LWP(distance_metric='cosine')\n",
    "lwp_model.fit(train_embeddings, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\190038340.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 2 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Using k=5 for kNN classification (based on top 1250 embeddings)...\n",
      "Dataset 2: Assigned pseudo-labels using kNN\n",
      "No prototypes available. Skipping update...\n",
      "Dataset 2: Updated prototypes for classes [None]\n",
      "Processing dataset 3 with kNN...\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "Using k=5 for kNN classification (based on top 1250 embeddings)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\190038340.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3: Assigned pseudo-labels using kNN\n",
      "Selecting top 1250 samples out of 2500 (percentage: 50.0%)...\n",
      "feature shape (X): (2500, 768)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 768 while Y.shape[1] == 1920000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[268], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Process D2-D10 with kNN and pseudo-labeling\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mprocess_dataset_with_knn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart_one_embed_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkd_lwp_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[265], line 43\u001b[0m, in \u001b[0;36mprocess_dataset_with_knn\u001b[1;34m(embed_dir, dataset_idx, lwp_model, kd_lwp_model, k, top_percentage)\u001b[0m\n\u001b[0;32m     41\u001b[0m lwp_model\u001b[38;5;241m.\u001b[39mfit(embeddings, all_pseudo_labels)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Update prototypes using top-2 pseudo-labels\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mkd_lwp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m kd_lwp_model\u001b[38;5;241m.\u001b[39mfit(embeddings)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# # Update prototypes (class centroids) with kNN pseudo-labeled samples\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# for label in np.unique(all_pseudo_labels):\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#     class_embeddings = embeddings[all_pseudo_labels == label]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#     if class_embeddings.size > 0:\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#         centroid = class_embeddings.mean(axis=0)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m#         lwp_model.prototypes[label] = centroid\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[263], line 110\u001b[0m, in \u001b[0;36mKnowledgeDistillationLWP.update_model\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_model\u001b[38;5;241m.\u001b[39mprototypes \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    105\u001b[0m         label: proto\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m label, proto \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_model\u001b[38;5;241m.\u001b[39mprototypes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    107\u001b[0m }\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Compute distillation loss\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m distillation_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistillation_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistillation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistillation_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Adjust prototypes based on distillation loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[263], line 45\u001b[0m, in \u001b[0;36mKnowledgeDistillationLWP.distillation_loss\u001b[1;34m(self, new_features)\u001b[0m\n\u001b[0;32m     43\u001b[0m old_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_model\u001b[38;5;241m.\u001b[39mpredict_proba(new_features)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Predictions from the current model\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m current_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlwp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Compute KL divergence\u001b[39;00m\n\u001b[0;32m     48\u001b[0m kl_div \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(old_predictions \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog((old_predictions \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m) \u001b[38;5;241m/\u001b[39m (current_predictions \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[258], line 72\u001b[0m, in \u001b[0;36mLWP.predict_proba\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     70\u001b[0m prob_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m---> 72\u001b[0m     distances \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     73\u001b[0m         label: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_fn(feature, proto) \u001b[38;5;28;01mfor\u001b[39;00m label, proto \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprototypes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     74\u001b[0m     }\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Convert distances to probabilities\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(distances\u001b[38;5;241m.\u001b[39mvalues())))  \u001b[38;5;66;03m# Exponential of negative distances\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[258], line 73\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     70\u001b[0m prob_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     72\u001b[0m     distances \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 73\u001b[0m         label: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistance_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label, proto \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprototypes\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     74\u001b[0m     }\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Convert distances to probabilities\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(distances\u001b[38;5;241m.\u001b[39mvalues())))  \u001b[38;5;66;03m# Exponential of negative distances\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[258], line 9\u001b[0m, in \u001b[0;36mLWP.<lambda>\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLWP\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learning Vector Prototypes with configurable distance function\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     DISTANCE_FUNCTIONS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y: np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(x \u001b[38;5;241m-\u001b[39m y),\n\u001b[1;32m----> 9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y: \u001b[43mcosine_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanhattan\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y: manhattan_distances(x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m: np\u001b[38;5;241m.\u001b[39mpower(np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mpower(np\u001b[38;5;241m.\u001b[39mabs(x \u001b[38;5;241m-\u001b[39m y), p)), \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mp)\n\u001b[0;32m     12\u001b[0m     }\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, distance_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdistance_params):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m            distance_params (dict): Additional parameters for the distance function\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\pairwise.py:1108\u001b[0m, in \u001b[0;36mcosine_distances\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine distance between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03mCosine distance is defined as 1.0 minus the cosine similarity.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;124;03m       [0.42..., 0.18...]])\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# 1.0 - cosine_similarity(X, Y) without copy\u001b[39;00m\n\u001b[1;32m-> 1108\u001b[0m S \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1109\u001b[0m S \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1110\u001b[0m S \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\pairwise.py:1657\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m \n\u001b[0;32m   1615\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1657\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1659\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\pairwise.py:189\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    187\u001b[0m         )\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    192\u001b[0m     )\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 768 while Y.shape[1] == 1920000"
     ]
    }
   ],
   "source": [
    "kd_lwp_model = KnowledgeDistillationLWP(alpha=0.5)\n",
    "\n",
    "# Process D2-D10 with kNN and pseudo-labeling\n",
    "for i in range(2, 11):\n",
    "    process_dataset_with_knn(part_one_embed_dir, i, lwp_model, kd_lwp_model, k=5, top_percentage=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on D1 (with ground truth)\n",
    "eval_labels_path = os.path.join('dataset', 'part_one_dataset', 'eval_data', '1_eval_data.tar.pth')\n",
    "eval_data = torch.load(eval_labels_path)\n",
    "eval_ground_truth = eval_data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_on_eval_embeddings(part_one_embed_dir, dataset_idx=1, model=lwp_model, ground_truth=eval_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on D2-D10\n",
    "for i in range(2, 11):\n",
    "    evaluate_on_eval_embeddings(part_one_embed_dir, dataset_idx=i, model=lwp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process D11-D20 (unlabeled datasets) with knowledge distillation\n",
    "for i in range(11, 21):\n",
    "    train_path = os.path.join(part_two_embed_dir, f'train_embeds_{i}.pt')\n",
    "    print(f\"Processing dataset D{i} from {train_path}...\")\n",
    "    \n",
    "    # Load train embeddings\n",
    "    train_embeddings = torch.load(train_path).numpy()\n",
    "    \n",
    "    # Perform knowledge distillation-based learning\n",
    "    kd_lwp_model.update_model(train_embeddings)\n",
    "    kd_lwp_model.fit(train_embeddings)\n",
    "    \n",
    "    # Evaluate or predict on eval set if needed\n",
    "    eval_path = os.path.join(part_two_embed_dir, f'eval_embeds_{i}.pt')\n",
    "    eval_embeddings = torch.load(eval_path).numpy()\n",
    "    predictions = kd_lwp_model.predict(eval_embeddings)\n",
    "    print(f\"Pseudo-labels for eval set of D{i}: {predictions[:10]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on D11-D20 (unlabeled datasets)\n",
    "for i in range(11, 21):\n",
    "    evaluate_on_eval_embeddings(part_two_embed_dir, dataset_idx=i, model=lwp_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
