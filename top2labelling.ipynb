{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import models, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('dataset', 'part_one_dataset', 'train_data')\n",
    "eval_dir = os.path.join('dataset', 'part_one_dataset', 'eval_data')\n",
    "train_path = os.path.join(train_dir, '1_train_data.tar.pth')\n",
    "eval_path = os.path.join(eval_dir, '1_eval_data.tar.pth')\n",
    "\n",
    "t = torch.load(train_path, weights_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_distances, manhattan_distances\n",
    "\n",
    "class LWP:\n",
    "    \"\"\"Learning Vector Prototypes with configurable distance function\"\"\"\n",
    "    \n",
    "    DISTANCE_FUNCTIONS = {\n",
    "        'euclidean': lambda x, y: np.linalg.norm(x - y),\n",
    "        'cosine': lambda x, y: cosine_distances(x.reshape(1, -1), y.reshape(1, -1))[0][0],\n",
    "        'manhattan': lambda x, y: manhattan_distances(x.reshape(1, -1), y.reshape(1, -1))[0][0],\n",
    "        'minkowski': lambda x, y, p=2: np.power(np.sum(np.power(np.abs(x - y), p)), 1/p)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, distance_metric='euclidean', **distance_params):\n",
    "        \"\"\"\n",
    "            distance_params (dict): Additional parameters for the distance function\n",
    "        \"\"\"\n",
    "        self.prototypes = {}\n",
    "        self.class_counts = {i: 0 for i in range(10)}\n",
    "        \n",
    "        if callable(distance_metric):\n",
    "            self.distance_fn = distance_metric\n",
    "        elif distance_metric in self.DISTANCE_FUNCTIONS:\n",
    "            if distance_metric == 'minkowski':\n",
    "                p = distance_params.get('p', 2)\n",
    "                self.distance_fn = lambda x, y: self.DISTANCE_FUNCTIONS[distance_metric](x, y, p)\n",
    "            else:\n",
    "                self.distance_fn = self.DISTANCE_FUNCTIONS[distance_metric]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distance metric: {distance_metric}. \" \n",
    "                           f\"Available metrics: {list(self.DISTANCE_FUNCTIONS.keys())}\")\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        unique_labels = np.unique(labels)\n",
    "        for label in unique_labels:\n",
    "            samples = features[labels == label]\n",
    "            num_samples = len(samples)\n",
    "            \n",
    "            if label not in self.prototypes:  # Original condition was: if label not in self.prototypes\n",
    "                self.prototypes[label] = samples.mean(axis=0)\n",
    "                self.class_counts[label] = len(samples)\n",
    "            else:\n",
    "                self.class_counts[label] += len(samples)\n",
    "                self.prototypes[label] = (\n",
    "                    (self.class_counts[label] - num_samples) / self.class_counts[label] * self.prototypes[label] +\n",
    "                    num_samples / self.class_counts[label] * samples.mean(axis=0)\n",
    "                )\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "    def predict(self, features):\n",
    "        preds = []\n",
    "        for feature in features:\n",
    "            distances = {\n",
    "                label: self.distance_fn(feature, proto)\n",
    "                for label, proto in self.prototypes.items()\n",
    "            }\n",
    "            preds.append(min(distances, key=distances.get))\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print (max(t['targets']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print ( t['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 3072)\n"
     ]
    }
   ],
   "source": [
    "data, targets = t['data'], t['targets'] \n",
    "data=data.reshape(data.shape[0], -1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, targets1= t['data'], t['targets']\n",
    "data1=data1.reshape(data1.shape[0], -1)\n",
    "data1=normalize(data1)\n",
    "dataloader=DataLoader(data1, batch_size=32, shuffle=False)\n",
    "lwp_model = LWP(distance_metric='cosine')  # LWP model with cosine distance\n",
    "lwp_model.fit(data1, targets1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypes after fit: {0: array([0.01729068, 0.01912501, 0.02102479, ..., 0.01516719, 0.0159201 ,\n",
      "       0.01601679]), 1: array([0.01837207, 0.01810205, 0.01747006, ..., 0.01770059, 0.01728502,\n",
      "       0.01624851]), 2: array([0.0164336 , 0.01742426, 0.01454287, ..., 0.01747445, 0.01748331,\n",
      "       0.01434357]), 3: array([0.01795165, 0.0166975 , 0.01529295, ..., 0.01780526, 0.01648547,\n",
      "       0.01536131]), 4: array([0.01489708, 0.01576391, 0.01352558, ..., 0.0190007 , 0.01918124,\n",
      "       0.01478338]), 5: array([0.01479913, 0.01467359, 0.01309324, ..., 0.01706052, 0.01606419,\n",
      "       0.01416187]), 6: array([0.0163735 , 0.01605917, 0.01306188, ..., 0.01921913, 0.01844561,\n",
      "       0.01541324]), 7: array([0.01802458, 0.01884868, 0.01836053, ..., 0.01945586, 0.01867545,\n",
      "       0.01490231]), 8: array([0.01797723, 0.02032782, 0.02269468, ..., 0.01198278, 0.01380034,\n",
      "       0.01474392]), 9: array([0.02226604, 0.02317505, 0.02398837, ..., 0.01793398, 0.01759059,\n",
      "       0.0167393 ])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Prototypes after fit:\", lwp_model.prototypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LWP fit...\n",
      "Prototypes initialized:\n",
      "Class 0: Prototype shape = (3072,)\n",
      "Class 1: Prototype shape = (3072,)\n",
      "Class 2: Prototype shape = (3072,)\n",
      "Class 3: Prototype shape = (3072,)\n",
      "Class 4: Prototype shape = (3072,)\n",
      "Class 5: Prototype shape = (3072,)\n",
      "Class 6: Prototype shape = (3072,)\n",
      "Class 7: Prototype shape = (3072,)\n",
      "Class 8: Prototype shape = (3072,)\n",
      "Class 9: Prototype shape = (3072,)\n"
     ]
    }
   ],
   "source": [
    "# Test LWP fit\n",
    "print(\"Testing LWP fit...\")\n",
    "lwp_model = LWP(distance_metric='cosine')\n",
    "lwp_model.fit(data1, targets1)\n",
    "\n",
    "# Check prototypes\n",
    "print(\"Prototypes initialized:\")\n",
    "for label, prototype in lwp_model.prototypes.items():\n",
    "    print(f\"Class {label}: Prototype shape = {prototype.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (2500, 3072)\n",
      "Labels shape: (2500,)\n",
      "Unique labels: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"Features shape:\", data1.shape)\n",
    "print(\"Labels shape:\", targets1.shape)\n",
    "print(\"Unique labels:\", np.unique(targets1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prototypes: {0: array([0.01729068, 0.01912501, 0.02102479, ..., 0.01516719, 0.0159201 ,\n",
      "       0.01601679]), 1: array([0.01837207, 0.01810205, 0.01747006, ..., 0.01770059, 0.01728502,\n",
      "       0.01624851]), 2: array([0.0164336 , 0.01742426, 0.01454287, ..., 0.01747445, 0.01748331,\n",
      "       0.01434357]), 3: array([0.01795165, 0.0166975 , 0.01529295, ..., 0.01780526, 0.01648547,\n",
      "       0.01536131]), 4: array([0.01489708, 0.01576391, 0.01352558, ..., 0.0190007 , 0.01918124,\n",
      "       0.01478338]), 5: array([0.01479913, 0.01467359, 0.01309324, ..., 0.01706052, 0.01606419,\n",
      "       0.01416187]), 6: array([0.0163735 , 0.01605917, 0.01306188, ..., 0.01921913, 0.01844561,\n",
      "       0.01541324]), 7: array([0.01802458, 0.01884868, 0.01836053, ..., 0.01945586, 0.01867545,\n",
      "       0.01490231]), 8: array([0.01797723, 0.02032782, 0.02269468, ..., 0.01198278, 0.01380034,\n",
      "       0.01474392]), 9: array([0.02226604, 0.02317505, 0.02398837, ..., 0.01793398, 0.01759059,\n",
      "       0.0167393 ])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Prototypes:\", lwp_model.prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([0.01729068, 0.01912501, 0.02102479, ..., 0.01516719, 0.0159201 ,\n",
      "       0.01601679]), 1: array([0.01837207, 0.01810205, 0.01747006, ..., 0.01770059, 0.01728502,\n",
      "       0.01624851]), 2: array([0.0164336 , 0.01742426, 0.01454287, ..., 0.01747445, 0.01748331,\n",
      "       0.01434357]), 3: array([0.01795165, 0.0166975 , 0.01529295, ..., 0.01780526, 0.01648547,\n",
      "       0.01536131]), 4: array([0.01489708, 0.01576391, 0.01352558, ..., 0.0190007 , 0.01918124,\n",
      "       0.01478338]), 5: array([0.01479913, 0.01467359, 0.01309324, ..., 0.01706052, 0.01606419,\n",
      "       0.01416187]), 6: array([0.0163735 , 0.01605917, 0.01306188, ..., 0.01921913, 0.01844561,\n",
      "       0.01541324]), 7: array([0.01802458, 0.01884868, 0.01836053, ..., 0.01945586, 0.01867545,\n",
      "       0.01490231]), 8: array([0.01797723, 0.02032782, 0.02269468, ..., 0.01198278, 0.01380034,\n",
      "       0.01474392]), 9: array([0.02226604, 0.02317505, 0.02398837, ..., 0.01793398, 0.01759059,\n",
      "       0.0167393 ])}\n"
     ]
    }
   ],
   "source": [
    "prototypes= lwp_model.prototypes\n",
    "print(prototypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Function to compute cosine similarity and select top samples\n",
    "def select_top_samples(embeddings, centroids, top_k=50):\n",
    "    \"\"\"\n",
    "    Select top-k% samples with highest cosine similarity to centroids.\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity(embeddings, centroids)\n",
    "    pseudo_labels = np.argmax(similarities, axis=1)\n",
    "    max_similarities = np.max(similarities, axis=1)\n",
    "\n",
    "    # Sort by similarity and select top k% samples\n",
    "    sorted_indices = np.argsort(max_similarities)[::-1]\n",
    "    top_count = len(sorted_indices) * top_k // 100\n",
    "    top_indices = sorted_indices[:top_count]\n",
    "\n",
    "    return embeddings[top_indices], pseudo_labels[top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_with_knn(embed_dir, dataset_idx, lwp_model, k=5, top_k=50):\n",
    "    print(f\"Processing dataset {dataset_idx} with kNN...\")\n",
    "\n",
    "    # Load embeddings\n",
    "    embed_path = os.path.join(embed_dir, f'train_embeds_{dataset_idx}.pt')\n",
    "    embeddings = torch.load(embed_path)\n",
    "\n",
    "    # Compute pseudo-labels and select top samples\n",
    "    centroids = np.vstack([proto for _, proto in sorted(lwp_model.prototypes.items())])\n",
    "    top_embeddings, top_pseudo_labels = select_top_samples(embeddings, centroids, top_k=top_k)\n",
    "\n",
    "    # Train kNN on the top samples\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn.fit(top_embeddings, top_pseudo_labels)\n",
    "\n",
    "    # Use kNN to assign pseudo-labels for all embeddings\n",
    "    all_pseudo_labels = knn.predict(embeddings)\n",
    "    print(f\"Dataset {dataset_idx}: Assigned pseudo-labels using kNN\")\n",
    "\n",
    "    # Update prototypes (class centroids) with kNN pseudo-labeled samples\n",
    "    for label in np.unique(all_pseudo_labels):\n",
    "        class_embeddings = embeddings[all_pseudo_labels == label]\n",
    "        if class_embeddings.size > 0:\n",
    "            centroid = class_embeddings.mean(axis=0)\n",
    "            lwp_model.prototypes[label] = centroid\n",
    "\n",
    "    print(f\"Dataset {dataset_idx}: Updated prototypes for classes {list(lwp_model.prototypes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\2342488638.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_embeddings = torch.load(train_embed_path)\n",
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\2342488638.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(eval_embed_path)\n"
     ]
    }
   ],
   "source": [
    "# Directories for embeddings\n",
    "part_one_embed_dir = 'part_1_vit_embeds'\n",
    "part_two_embed_dir = 'part_2_vit_embeds'\n",
    "\n",
    "# Load D1 embeddings and targets\n",
    "train_embed_path = os.path.join(part_one_embed_dir, 'train_embeds_1.pt')\n",
    "eval_embed_path = os.path.join(part_one_embed_dir, 'eval_embeds_1.pt')\n",
    "train_embeddings = torch.load(train_embed_path)\n",
    "eval_embeddings = torch.load(eval_embed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\782853816.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(train_path)\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join('dataset', 'part_one_dataset', 'train_data', '1_train_data.tar.pth')\n",
    "data = torch.load(train_path)\n",
    "targets = data['targets']\n",
    "\n",
    "# Initialize and fit LWP model\n",
    "lwp_model = LWP(distance_metric='cosine')\n",
    "lwp_model.fit(train_embeddings, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 2 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 2: Assigned pseudo-labels using kNN\n",
      "Dataset 2: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 3 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 3: Assigned pseudo-labels using kNN\n",
      "Dataset 3: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 4 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 4: Assigned pseudo-labels using kNN\n",
      "Dataset 4: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 5 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 5: Assigned pseudo-labels using kNN\n",
      "Dataset 5: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 6 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 6: Assigned pseudo-labels using kNN\n",
      "Dataset 6: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 7 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 7: Assigned pseudo-labels using kNN\n",
      "Dataset 7: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 8 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 8: Assigned pseudo-labels using kNN\n",
      "Dataset 8: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 9 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 9: Assigned pseudo-labels using kNN\n",
      "Dataset 9: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Processing dataset 10 with kNN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\3176197651.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 10: Assigned pseudo-labels using kNN\n",
      "Dataset 10: Updated prototypes for classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 11):\n",
    "    process_dataset_with_knn(part_one_embed_dir, i, lwp_model, k=5, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(11, 21):\n",
    "    process_dataset_with_knn(part_two_embed_dir, i, lwp_model, k=5, top_k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_data = torch.load(eval_labels_path)\n",
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on dataset 1...\n",
      "Accuracy on eval set 1: 86.56%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       252\n",
      "           1       0.89      0.95      0.92       217\n",
      "           2       0.99      0.65      0.79       264\n",
      "           3       0.71      0.90      0.79       242\n",
      "           4       0.77      0.89      0.83       257\n",
      "           5       0.93      0.71      0.81       252\n",
      "           6       0.95      0.92      0.93       269\n",
      "           7       0.77      0.90      0.83       233\n",
      "           8       0.95      0.93      0.94       266\n",
      "           9       0.89      0.95      0.92       248\n",
      "\n",
      "    accuracy                           0.87      2500\n",
      "   macro avg       0.88      0.87      0.86      2500\n",
      "weighted avg       0.88      0.87      0.86      2500\n",
      "\n",
      "Evaluating on dataset 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 2: [3 5 0 1 0 0 7 8 8 0]...\n",
      "Evaluating on dataset 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 3: [3 2 7 8 7 4 9 0 6 0]...\n",
      "Evaluating on dataset 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 4: [4 5 7 7 2 3 2 4 5 8]...\n",
      "Evaluating on dataset 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 5: [1 4 6 1 8 3 0 3 4 7]...\n",
      "Evaluating on dataset 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 6: [4 4 0 6 6 7 1 8 6 1]...\n",
      "Evaluating on dataset 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 7: [6 8 1 7 3 0 8 7 0 3]...\n",
      "Evaluating on dataset 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 8: [3 5 4 2 0 1 1 4 5 9]...\n",
      "Evaluating on dataset 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 9: [7 3 8 5 3 7 8 1 3 6]...\n",
      "Evaluating on dataset 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANUSHKA SINGH\\AppData\\Local\\Temp\\ipykernel_27144\\917197337.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_embeddings = torch.load(embed_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on eval set 10: [2 9 3 4 9 5 8 3 4 5]...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Function to evaluate on eval embeddings\n",
    "def evaluate_on_eval_embeddings(embed_dir, dataset_idx, model, ground_truth=None):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model using eval embeddings.\n",
    "    Args:\n",
    "        embed_dir (str): Path to the directory containing eval embeddings.\n",
    "        dataset_idx (int): Dataset index (e.g., 1 for D1, 2 for D2, etc.).\n",
    "        model (LWP or kNN): Trained model for predictions.\n",
    "        ground_truth (np.array): True labels for eval set (if available).\n",
    "    Returns:\n",
    "        dict: Evaluation metrics (if ground_truth provided).\n",
    "    \"\"\"\n",
    "    # Load eval embeddings\n",
    "    embed_path = os.path.join(embed_dir, f'eval_embeds_{dataset_idx}.pt')\n",
    "    eval_embeddings = torch.load(embed_path)\n",
    "\n",
    "    # Predict labels using the trained model\n",
    "    print(f\"Evaluating on dataset {dataset_idx}...\")\n",
    "    predicted_labels = model.predict(eval_embeddings)\n",
    "\n",
    "    if ground_truth is not None:\n",
    "        # Compute evaluation metrics if ground truth is available\n",
    "        accuracy = accuracy_score(ground_truth, predicted_labels)\n",
    "        report = classification_report(ground_truth, predicted_labels, zero_division=0)\n",
    "        print(f\"Accuracy on eval set {dataset_idx}: {accuracy * 100:.2f}%\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        return {\"accuracy\": accuracy, \"report\": report}\n",
    "    else:\n",
    "        # No ground truth available\n",
    "        print(f\"Predictions on eval set {dataset_idx}: {predicted_labels[:10]}...\")\n",
    "        return {\"predicted_labels\": predicted_labels}\n",
    "\n",
    "# Directories for eval embeddings\n",
    "part_one_eval_dir = 'part_1_vit_embeds'\n",
    "part_two_eval_dir = 'part_2_vit_embeds'\n",
    "\n",
    "# Evaluate on D1 (with ground truth)\n",
    "eval_labels_path = os.path.join('dataset', 'part_one_dataset', 'eval_data', '1_eval_data.tar.pth')\n",
    "eval_data = torch.load(eval_labels_path)\n",
    "eval_ground_truth = eval_data['targets']\n",
    "\n",
    "# Evaluate on D1 using LWP model\n",
    "evaluate_on_eval_embeddings(part_one_eval_dir, dataset_idx=1, model=lwp_model, ground_truth=eval_ground_truth)\n",
    "\n",
    "# Evaluate on D2-D10 (unlabeled datasets)\n",
    "for i in range(2, 11):\n",
    "    evaluate_on_eval_embeddings(part_one_eval_dir, dataset_idx=i, model=lwp_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on D11-D20 (unlabeled datasets with different distribution)\n",
    "for i in range(11, 21):\n",
    "    evaluate_on_eval_embeddings(part_two_eval_dir, dataset_idx=i, model=lwp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Train LWP model on unlabeled \n",
    "\n",
    "# for i in range(2,11):\n",
    "#     train_path=os.path.join(train_dir, f'{i}_train_data.tar.pth')\n",
    "#     print(f\"Processing dataset {i} from {train_path}\")\n",
    "\n",
    "#     dataset=torch.load(train_path, weights_only=False)\n",
    "#     data= dataset['data']\n",
    "#     data=data.reshape(-1,3072)\n",
    "#     print(f\"Reshaped dataset shape: {data.shape}\")  # Should be (N, 3072)\n",
    "\n",
    "#     data = normalize(data.reshape(data.shape[0],-1))  # Normalize data (important for distance calculations)\n",
    "#     # Prepare DataLoader for the dataset\n",
    "#     tensor_dataset = torch.tensor(data, dtype=torch.float32)\n",
    "#     dataloader = DataLoader(data, batch_size=32, shuffle=False)\n",
    "#     confidence_scores = []\n",
    "#     embeddings = []\n",
    "#     predictions = []\n",
    "#     for batch in dataloader:\n",
    "#         print(f\"Batch shape: {batch.shape}\")  # Should be (batch_size, 3072)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataloader:\n",
    "\n",
    "#             inputs = batch.numpy()\n",
    "#             print(inputs.shape)\n",
    "#             batch_predictions = []\n",
    "#             batch_distances = []\n",
    "#             print(\"Prototypes:\", lwp_model.prototypes)\n",
    "\n",
    "#             # Predict pseudo-labels and calculate distances to prototypes\n",
    "#             for sample in inputs:\n",
    "#                 print(f\"Sample shape before flattening: {sample.shape}\")\n",
    "#                 sample = sample.flatten()  # Ensure the sample is a 1D vector\n",
    "#                 print(f\"Sample shape after flattening: {sample.shape}\")\n",
    "#                 dist_to_prototypes = {label: lwp_model.distance_fn(sample, proto) for label, proto in lwp_model.prototypes.items()}\n",
    "#                 closest_label = min(dist_to_prototypes, key=dist_to_prototypes.get)\n",
    "#                 closest_distance = dist_to_prototypes[closest_label]\n",
    "#                 batch_predictions.append(closest_label)\n",
    "#                 batch_distances.append(closest_distance)\n",
    "\n",
    "#             predictions.extend(batch_predictions)\n",
    "#             confidence = 1 / (1 + np.array(batch_distances))  # Convert distances to confidence scores\n",
    "#             confidence_scores.extend(confidence.tolist())\n",
    "#             embeddings.extend(inputs)\n",
    "\n",
    "#     embeddings = np.array(embeddings)\n",
    "\n",
    "#     # Step 4: Select top 50% most confident samples\n",
    "#     sorted_indices = np.argsort(confidence_scores)[::-1]  # Sort by confidence scores (descending)\n",
    "#     top_50_percent_indices = sorted_indices[:len(sorted_indices) // 2]\n",
    "\n",
    "#     top_50_embeddings = embeddings[top_50_percent_indices]\n",
    "#     top_50_predictions = np.array(predictions)[top_50_percent_indices]\n",
    "\n",
    "#     # Step 5: Construct class centroids\n",
    "#     class_centroids = {}\n",
    "#     for label in np.unique(top_50_predictions):\n",
    "#         class_embeddings = top_50_embeddings[top_50_predictions == label]\n",
    "#         if class_embeddings.size > 0:\n",
    "#             centroid = class_embeddings.mean(axis=0)\n",
    "#             class_centroids[label] = centroid\n",
    "\n",
    "#     # Print centroids\n",
    "#     print(f\"Class centroids calculated for dataset {i}:\")\n",
    "#     for label, centroid in class_centroids.items():\n",
    "#         print(f\"Class {label}: Centroid = {centroid[:5]}...\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
