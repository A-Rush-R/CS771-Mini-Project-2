{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 48,
=======
   "execution_count": 1,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
<<<<<<< Updated upstream
    "import os\n",
    "from torchvision import datasets, transforms, models"
=======
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import numpy as np"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 49,
=======
   "execution_count": 2,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('dataset', 'part_one_dataset', 'train_data')\n",
<<<<<<< Updated upstream
    "eval_dir = os.path.join('dataset', 'part_one_dataset', 'eval_data')\n",
=======
    "eval_dir = os.path.join('dataset', 'part_one_dataset', 'eval_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the keys are dict_keys(['data', 'targets'])\n"
     ]
    }
   ],
   "source": [
>>>>>>> Stashed changes
    "\n",
    "train_path = os.path.join(train_dir, '1_train_data.tar.pth')\n",
    "eval_path = os.path.join(eval_dir, '1_eval_data.tar.pth')\n",
    "\n",
<<<<<<< Updated upstream
    "t = torch.load(train_path, weights_only = False)\n"
=======
    "t = torch.load(train_path, weights_only = False)\n",
    "print('the keys are', t.keys())"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove the last layer\n",
    "resnet.eval()  # Set to evaluation mode\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Example ndarray of shape (2500, 32, 32, 3)\n",
    "data, targets = t['data'], t['targets'] # both numpy.ndarray\n",
    "# Convert to PyTorch tensor\n",
    "X_tensor = torch.tensor(data, dtype=torch.float32)  # Convert to tensor\n",
    "X_tensor = X_tensor.permute(0, 3, 1, 2)  # Change shape to (2500, 3, 32, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
=======
   "execution_count": 4,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "torch.Size([2500, 3, 224, 224])\n"
=======
      "the keys are dict_keys(['data', 'targets'])\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 (ResNet input size)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# 2. Apply the transformation to each image in the batch\n",
    "# Convert the tensor to float32 if it's not already (for normalization)\n",
    "tensor = X_tensor.float()\n",
    "\n",
    "# 3. Apply transformations to the batch of images\n",
    "# The transform function expects each image to be a PIL Image,\n",
    "# so we will need to loop through and apply the transformations to each image\n",
    "transformed_images = []\n",
    "for image in tensor:\n",
    "    # Convert each image tensor (C, H, W) to PIL Image for transformation\n",
    "    transformed_image = transform(image)  # Apply the transformations\n",
    "    transformed_images.append(transformed_image)\n",
    "\n",
    "# 4. Stack the transformed images back into a batch\n",
    "preprocessed_tensor = torch.stack(transformed_images)  # Shape: (2500, 3, 224, 224)\n",
    "\n",
    "# 5. Check the shape of the preprocessed tensor\n",
    "print(preprocessed_tensor.shape)  "
=======
    "e = torch.load(eval_path, weights_only = False)\n",
    "print('the keys are', e.keys())"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 54,
=======
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "\n",
    "for i in range (1, 11) :\n",
    "    train_path = os.path.join(train_dir, f'{i}_train_data.tar.pth')\n",
    "    data = torch.load(train_path, weights_only = False)\n",
    "    datasets.append((data['data'].reshape(-1,3,32,32), data['targets'] if 'targets' in data else np.zeros(data['data'].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PretrainedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, feature_dim=512, model_name='resnet18'):\n",
    "        super(PretrainedFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Choose backbone model\n",
    "        if model_name == 'resnet18':\n",
    "            backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        elif model_name == 'resnet50':\n",
    "            backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        elif model_name == 'efficientnet_b0':\n",
    "            backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        elif model_name == 'vit_b_16':\n",
    "            backbone = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        if model_name.startswith('resnet'):\n",
    "            self.features = nn.Sequential(*list(backbone.children())[:-1])\n",
    "            self.feature_size = backbone.fc.in_features\n",
    "        elif model_name.startswith('efficientnet'):\n",
    "            self.features = nn.Sequential(*list(backbone.children())[:-1])\n",
    "            self.feature_size = backbone.classifier[1].in_features\n",
    "        elif model_name.startswith('vit'):\n",
    "            self.features = backbone\n",
    "            self.feature_size = backbone.heads[0].in_features\n",
    "        \n",
    "        # Freeze the pre-trained layers\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Define image preprocessing\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Ensure input is in the correct format and range\n",
    "        print(x.shape)\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        print(x.shape)\n",
    "        # Apply preprocessing\n",
    "        x = self.preprocess(x)\n",
    "        \n",
    "        # Extract features\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Flatten if needed\n",
    "        if x.dim() > 2:\n",
    "            x = torch.flatten(x, 1)\n",
    "            \n",
    "        # Apply adapter if needed\n",
    "        x = self.adapter(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = datasets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningWithPrototype:\n",
    "    def __init__(self, n_classes, feature_dim=512, learning_rate=0.001, model_name='resnet18', batch_size=32):\n",
    "        self.n_classes = n_classes\n",
    "        self.feature_dim = feature_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.prototypes = None\n",
    "        self.feature_extractor = None\n",
    "        \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize the pre-trained feature extractor\"\"\"\n",
    "        self.feature_extractor = PretrainedFeatureExtractor(\n",
    "            feature_dim=self.feature_dim,\n",
    "            model_name=self.model_name\n",
    "        )\n",
    "        \n",
    "        # Freeze all parameters\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def extract_features(self, images):\n",
    "        \"\"\"Convert images to features using the feature extractor\"\"\"\n",
    "        self.feature_extractor.eval()\n",
    "        features_list = []\n",
    "        \n",
    "        # Create DataLoader for batch processing\n",
    "        dataset = TensorDataset(torch.FloatTensor(images))\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch_features = self.feature_extractor(batch[0])\n",
    "                features_list.append(batch_features)\n",
    "                \n",
    "        return torch.cat(features_list, dim=0).numpy()\n",
    "    \n",
    "    def compute_prototypes(self, features, labels):\n",
    "        \"\"\"Compute class prototypes from features\"\"\"\n",
    "        prototypes = np.zeros((self.n_classes, self.feature_dim))\n",
    "        for i in range(self.n_classes):\n",
    "            if np.sum(labels == i) > 0:\n",
    "                prototypes[i] = np.mean(features[labels == i], axis=0)\n",
    "        return prototypes\n",
    "    \n",
    "    def predict(self, features):\n",
    "        \"\"\"Predict labels based on distance to prototypes\"\"\"\n",
    "        distances = euclidean_distances(features, self.prototypes)\n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def fit_predict_iterate(self, datasets):\n",
    "        \"\"\"Iterative training and prediction on multiple datasets\"\"\"\n",
    "        all_predictions = []\n",
    "        \n",
    "        # Initialize model\n",
    "        self.initialize_model()\n",
    "        \n",
    "        # Initial feature extraction and prototype computation\n",
    "        X_init, y_init = datasets[0]\n",
    "        features_init = self.extract_features(X_init)\n",
    "        self.prototypes = self.compute_prototypes(features_init, y_init)\n",
    "        \n",
    "        # Store predictions for first dataset\n",
    "        all_predictions.append(y_init)\n",
    "        \n",
    "        # Iterate through remaining datasets\n",
    "        for i, (X_next, _) in enumerate(datasets[1:], 1):\n",
    "            print(f\"\\nProcessing dataset {i+1}\")\n",
    "            \n",
    "            # Extract features for current dataset\n",
    "            features_next = self.extract_features(X_next)\n",
    "            \n",
    "            # Predict labels using current prototypes\n",
    "            predicted_labels = self.predict(features_next)\n",
    "            all_predictions.append(predicted_labels)\n",
    "            \n",
    "            # Update prototypes with new features\n",
    "            features_next = self.extract_features(X_next)\n",
    "            self.prototypes = self.compute_prototypes(features_next, predicted_labels)\n",
    "        \n",
    "        return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Embeddings shape: <class 'torch.Tensor'>\n"
=======
      "10\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PretrainedFeatureExtractor' object has no attribute 'adapter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(n_classes)\n\u001b[1;32m     13\u001b[0m lwp \u001b[38;5;241m=\u001b[39m LearningWithPrototype(\n\u001b[1;32m     14\u001b[0m     n_classes\u001b[38;5;241m=\u001b[39mn_classes,\n\u001b[1;32m     15\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mlwp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict_iterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 60\u001b[0m, in \u001b[0;36mLearningWithPrototype.fit_predict_iterate\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Initial feature extraction and prototype computation\u001b[39;00m\n\u001b[1;32m     59\u001b[0m X_init, y_init \u001b[38;5;241m=\u001b[39m datasets[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m features_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprototypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_prototypes(features_init, y_init)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Store predictions for first dataset\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[77], line 33\u001b[0m, in \u001b[0;36mLearningWithPrototype.extract_features\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 33\u001b[0m         batch_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         features_list\u001b[38;5;241m.\u001b[39mappend(batch_features)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(features_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/771/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/771/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[74], line 53\u001b[0m, in \u001b[0;36mPretrainedFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Apply adapter if needed\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapter\u001b[49m(x)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/771/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PretrainedFeatureExtractor' object has no attribute 'adapter'"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "device = torch.device(\"mps\") # for apple. aap apna dekh lijye\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "preprocessed_batch = preprocessed_tensor.to(device)\n",
    "\n",
    "# 4. Get the embeddings (feature maps)\n",
    "with torch.no_grad():  # Disable gradients for inference\n",
    "    feature_maps = resnet(preprocessed_batch)  # Shape will be (batch_size, 512, 1, 1)\n",
    "\n",
    "# 5. Flatten the feature maps (optional)\n",
    "embeddings = feature_maps.view(feature_maps.size(0), -1)  # Flatten to shape (batch_size, embedding_size)\n",
    "\n",
    "# Print the embeddings shape (should be 2500 x 512)\n",
    "print(\"Embeddings shape:\", type(embeddings))"
=======
    "\n",
    "\"\"\"\n",
    "Process multiple datasets using Learning with Prototype with pre-trained feature extractor\n",
    "\n",
    "Parameters:\n",
    "datasets: List of tuples (X, y) where X is the image data and y is labels\n",
    "            First dataset should have labels, others can have dummy labels\n",
    "model_name: Name of the pre-trained model to use ('resnet18', 'resnet50', \n",
    "            'efficientnet_b0', or 'vit_b_16')\n",
    "\"\"\"\n",
    "# Initialize LwP model\n",
    "n_classes = len(np.unique(datasets[0][1]))\n",
    "print(n_classes)\n",
    "lwp = LearningWithPrototype(\n",
    "    n_classes=n_classes,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "predictions = lwp.fit_predict_iterate(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(datasets[0][1])"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< Updated upstream
   "source": [
    "def learning_with_prototype(X_train, y_train, X_test):\n",
    "\n",
    "    # needs to be tweaked atm\n",
    "    \"\"\"\n",
    "    Learning with Prototype (LwP) classifier.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (ndarray): Training data features.\n",
    "        y_train (ndarray): Training data labels.\n",
    "        X_test (ndarray): Test data features.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Predicted labels for the test data.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(y_train)\n",
    "    class_prototypes = [np.mean(X_train[y_train == label], axis=0) for label in unique_labels]\n",
    "    predicted_labels = [unique_labels[np.argmin(np.linalg.norm(sample - class_prototypes, axis=1))] for sample in X_test]\n",
    "    return np.array(predicted_labels)\n"
   ]
=======
   "source": []
>>>>>>> Stashed changes
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": "cs771",
=======
   "display_name": "771",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.12.6"
=======
   "version": "3.12.5"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
